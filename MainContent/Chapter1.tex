\chapter{\leavevmode\newline Introduction}\label{chap:Chapter_1}

When a vulnerability surfaced within federal information systems in late 2023, security teams across multiple agencies found themselves in a desperate race against time. Defenders labored to identify every affected component, racing to understand what adversarial threat actors were already exploiting~\cite{Ivanti_Threat_Impact}. Yet they possessed no comprehensive understanding of how vulnerabilities in one system could cascade across interconnected infrastructure and national security systems. Weeks passed while agencies struggled to map the blast radius of potential compromise. During this time adversaries retained the initiative, because existing documentation bore no faithful resemblance to the agencies' actual infrastructure configurations~\cite{CISA_Emergency_2023}. This operational failure stands not as an isolated incident but as an exemplar of the challenges that modern enterprises confront when managing complex information systems while simultaneously maintaining effective information assurance postures.

The consequences of such failures extend beyond the immediate organizations affected. When defenders cannot comprehend the cascading impacts of compromise, risk communication to organizational leadership degrades, remediation prioritization loses connection to actual impact severity, and defensive coordination across organizational boundaries becomes impractical. The inability to understand system interdependencies transforms what might be contained incidents into enterprise-wide crises. Security teams find themselves engaged in reactive firefighting rather than proactive defense, expending resources on manual discovery efforts that model-based approaches are designed to accomplish with substantially greater efficiency~\cite{DoD_DE_Strategy_2018}.

Information assurance, as codified by the National Institute of Standards and Technology, encompasses those measures that protect and defend information and information systems by ensuring their availability, integrity, authentication, confidentiality, and non-repudiation~\cite{Force_2020}. Cybersecurity constitutes an operational component within this broader discipline, concentrating specifically upon the protection of information systems from unauthorized access, use, disclosure, disruption, modification, or destruction. Throughout this dissertation, the term \textit{information assurance} denotes the broader discipline encompassing security policy, risk management, compliance verification, and protective measures. The term \textit{cybersecurity} refers specifically to the technical and operational dimensions of protecting systems from cyber threats. These terms serve distinct purposes and shall not be employed interchangeably: information assurance represents the broader governance and assurance framework, while cybersecurity addresses the specific protective mechanisms and threat responses that operate within that framework.

Terminological precision bears operational consequences. Organizations that conflate information assurance with cybersecurity often underinvest in the governance, documentation, and architectural foundations upon which effective cybersecurity operations depend. The failure to maintain accurate system documentation, for example, represents an information assurance shortfall that manifests as cybersecurity operational degradation. Literature from defense and aerospace contexts suggests that Digital Engineering may address both dimensions: the governance and documentation requirements of information assurance and the operational visibility requirements of cybersecurity defense. Whether IT and information assurance professionals recognize this potential constitutes a central question of this research.

This chapter examines the challenges organizations encounter when implementing information assurance practices and managing information technology (IT) service delivery, introducing Digital Engineering as a disciplinary approach capable of addressing gaps that persist despite mature frameworks including the National Institute of Standards and Technology (NIST) Risk Management Framework (RMF)~\cite{Force_2020}, the Information Technology Infrastructure Library (ITIL)~\cite{Cannon_AXELOS_2013}, and the Unified Architecture Framework (UAF)~\cite{OMG_UAF_2022}.

\section{Current State of Information System Management}

Organizations today operate within an environment defined by relentless technological evolution and escalating system complexity. The convergence of cloud computing, microservices architectures, Internet of Things (IoT) devices, and operational technology has spawned intricate webs of interdependencies that overwhelm traditional approaches to both information assurance and IT service delivery~\cite{Benbya_Nan_Tanriverdi_Yoo_2020}. These technological advances deliver undeniable operational benefits. But they exact a heavy toll in system visibility, security control implementation, configuration management, and service delivery coordination.

The pace of technological change continues to accelerate. Organizations that required months to deploy new capabilities a decade ago now deploy changes continuously through automated pipelines. This acceleration benefits operational agility but strains the documentation and verification processes upon which information assurance depends. Static documentation approaches designed for quarterly or annual update cycles cannot maintain accuracy when systems change hourly. The structural mismatch between documentation velocity and operational velocity creates systematic failures that compound over time.

Enterprise information systems now routinely span multiple technology domains: cloud-based infrastructure and services, on-premises data centers, edge computing environments, operational technology networks, and mobile and remote access systems. This technological heterogeneity generates persistent challenges in maintaining security visibility, implementing consistent protection mechanisms, and delivering reliable IT services across disparate environments. System dependency tracking operates without confidence; configuration management falters; security control implementation proceeds inconsistently across heterogeneous platforms~\cite{Bokan_Santos_2021}.

The challenge extends beyond mere technical complexity. Organizational structures that evolved to manage discrete technology domains now impede the integrated visibility that modern environments require. Security teams operate separately from IT operations teams, while cloud architects work independently of network engineers. Application developers deploy services without understanding infrastructure dependencies. This organizational fragmentation mirrors and reinforces the technical fragmentation that undermines both information assurance and IT service delivery effectiveness.

\subsection{Information Assurance Practice}

The practice of information assurance has evolved in response to the complexities of modern enterprise environments. The NIST Risk Management Framework provides a structured, disciplined approach for managing security and privacy risk that organizations can apply across diverse information systems~\cite{Force_2020}. The RMF establishes a lifecycle approach to security through seven iterative steps: prepare, categorize, select, implement, assess, authorize, and monitor. This framework has become foundational for federal agencies and finds increasing adoption among organizations operating national security systems and private enterprises seeking systematic approaches to information assurance.

The RMF represents a significant advancement over earlier compliance-focused approaches that treated security as a point-in-time certification rather than a continuous process. The framework's emphasis upon continuous monitoring and ongoing authorization reflects recognition that security postures change constantly as systems evolve, threats emerge, and organizational requirements shift. Effective implementation of continuous monitoring requires capabilities that most organizations lack: real-time visibility into system configurations, automated assessment of control effectiveness, and dynamic risk calculation based upon current rather than documented system states.

Additional information assurance lifecycle frameworks exist including ISO 31000~\cite{for_2018}, the NIST Cybersecurity Framework~\cite{Cybersecurity_Framework_2013}, and COBIT~\cite{ISACA2019COBIT}, among others. These frameworks provide alternative approaches to the RMF\@. However, they share common challenges in maintaining accurate documentation, ensuring visibility into system states, and coordinating security efforts across organizational boundaries. The approach taken by this research focuses upon the NIST Risk Management Framework, which reduces complexity by avoiding direct comparison among multiple frameworks while still addressing challenges common to all.

Security control selection represents a key RMF activity. Federal information systems and national security systems typically utilize NIST Special Publication 800-53 Revision 5 as the authoritative catalog of security controls~\cite{Force_2020}. Organizations operating outside federal requirements may employ alternative frameworks for control selection, including ISO/IEC 27001~\cite{ISO27001-2023}, the NIST Cybersecurity Framework, or industry-specific standards. The methodology presented in this research focuses upon NIST 800-53 Revision 5 given its applicability to federal and national security contexts, though the underlying principles extend to organizations employing other control frameworks.

The selection of appropriate security controls depends upon accurate understanding of the systems being protected, their operational context, their interconnections with other systems, and their role within the broader enterprise architecture. Control selection that proceeds from inaccurate system understanding produces security postures that address documented rather than actual risk. This disconnect between documentation and reality represents a structural challenge that persists regardless of which control framework an organization employs.

Implementing the RMF effectively presents documented challenges in complex technological environments. Organizations must categorize information systems based upon potential impact, select appropriate security controls from comprehensive catalogs, implement those controls across diverse platforms, assess control effectiveness, obtain authorization decisions, and maintain continuous monitoring throughout the system lifecycle. Each step demands accurate, current information about system configurations, security control implementations, and operational states. Traditional documentation approaches struggle to maintain such information.

The continuous monitoring requirement deserves particular attention because it exposes the limitations of document-centric approaches most directly. Continuous monitoring as envisioned by the RMF requires ongoing awareness of security-relevant system changes, automated assessment of security posture impacts, and timely reporting to authorizing officials. Organizations attempting to implement continuous monitoring through manual processes discover that the labor required exceeds available resources. Organizations attempting to implement continuous monitoring through automation discover that they lack the authoritative system models and configuration baselines that automation requires.

\subsection{IT Service Management Practice}

IT service management (ITSM) has matured through frameworks designed to ensure reliable service delivery across the enterprise. The Information Technology Infrastructure Library provides comprehensive guidance for aligning IT services with business needs through structured processes for service strategy, service design, service transition, service operation, and continual service improvement~\cite{Cannon_AXELOS_2013}. ITIL emphasizes configuration management, change management, and service asset management as foundational capabilities upon which effective IT service delivery depends.

The evolution from ITIL Version 3 to ITIL 4 reflects recognition that service management practices must adapt to cloud computing, DevOps practices, and agile delivery models. ITIL 4 introduces the Service Value System concept, emphasizing flexibility and continuous improvement over rigid process compliance. Yet the core dependencies upon accurate configuration information and effective change coordination persist regardless of which ITIL version organizations adopt. The Service Value System cannot create value if the underlying information about services, configurations, and dependencies remains inaccurate or incomplete.

Configuration management within the ITIL framework requires organizations to maintain accurate configuration management databases documenting configuration items, their attributes, and their relationships. Change management processes depend upon accurate configuration information to assess change impacts and coordinate modifications across interconnected systems. Service asset management extends these capabilities to encompass the full lifecycle of IT assets from acquisition through retirement. These interconnected processes provide structure for managing complex IT environments. But they depend upon the accuracy and currency of underlying information---accuracy that organizations consistently fail to achieve.

The relationship between configuration management and change management illustrates the compounding nature of documentation failures. Change management processes assess proposed changes against documented configurations and relationships. When documentation is incomplete, change assessments miss dependencies that exist in operational systems. Changes approved based upon incomplete assessments cause unintended impacts. Those impacts require emergency changes to address. Emergency changes bypass change management processes, further degrading documentation accuracy. This cycle perpetuates itself, progressively undermining both configuration management and change management effectiveness.

\subsection{Challenges in Current Practice}

Traditional documentation approaches and manual tracking methods prove increasingly inadequate for capturing and managing the complexity inherent in modern information systems. Paper-based security documentation, static network diagrams, and periodic compliance assessments fail to reflect the dynamic nature of contemporary enterprise environments. IT service management practices that rely upon manual configuration tracking and change coordination struggle to maintain accuracy and timeliness in environments characterized by continuous deployment and rapid change cycles.

The structural challenge lies not in the quality of frameworks or the dedication of practitioners. The challenge lies in the mismatch between the documentation velocity that manual processes can sustain and the operational velocity that modern enterprise environments demand. No amount of process improvement or additional staffing can close this gap using traditional approaches. The solution requires a paradigm shift from document-centric to model-centric practices---precisely the shift that Digital Engineering provides.

Research documents pervasive failures across both information assurance and IT service management domains. Industry analysts report that eighty percent of Configuration Management Database (CMDB) implementations fail to deliver intended value~\cite{Gartner_CMDB_2019}. Studies find that organizations can monitor only sixty-six percent of their IT environments, leaving thirty-four percent unmonitored~\cite{IDC_Exabeam_2023}. Shadow IT---technology acquired or deployed outside official governance---now represents thirty to forty percent of enterprise IT spending, creating assets invisible to documentation efforts~\cite{Gartner_Shadow_IT_2022}. The mean time to identify security breaches averages 204 days, reflecting the visibility gaps that impair threat detection~\cite{IBM_Ponemon_2024}.

These statistics represent not merely organizational shortcomings but systemic limitations of document-centric approaches. Organizations that invest heavily in documentation still experience these failures. Organizations with mature governance processes still discover undocumented systems and unknown dependencies. The consistency of these failures across diverse organizations suggests that the problem lies not in execution but in approach.

These failures share a common pattern: organizations cannot maintain accurate, current documentation of their information systems using traditional approaches. The rate of change in modern IT environments exceeds the capacity of manual documentation processes. Static artifacts become obsolete before completion. Configuration databases diverge from operational reality. Security documentation describes intended states rather than actual implementations. This documentation-reality gap undermines every process that depends upon accurate system information---which includes nearly all information assurance and IT service management activities.

Chapter 2 examines these visibility and documentation failures in depth, synthesizing peer-reviewed research and industry analysis to establish the evidence base for why traditional practices fail. Documentation challenges reflect multiple converging factors: organizational silos fragmenting visibility, complexity exceeding human documentation capacity, manual processes unable to match rates of change, and technical debt accumulating in documentation domains.

\section{Digital Engineering as Potential Solution}

Digital Engineering represents a systematic approach to designing, developing, and managing complex systems through integrated digital models and data-driven processes~\cite{DigitalEngineeringInformationExchange}. Originally forged within the defense and aerospace sectors, Digital Engineering has been formalized through authoritative guidance from organizations including the United States Department of Defense (DoD)~\cite{DoD_Engineering_2022}, the National Aeronautics and Space Administration (NASA)~\cite{Engineer}, and the International Council on Systems Engineering (INCOSE)~\cite{DigitalEngineeringInformationExchange}. While these foundational practices emerged primarily from physical systems engineering, the underlying principles offer capabilities of documented value for information technology and information assurance domains.

The emergence of Digital Engineering from defense and aerospace contexts carries significance beyond historical interest. These sectors developed Digital Engineering practices to address challenges structurally similar to those confronting enterprise IT and Information Assurance: complex interdependent systems, stringent compliance requirements, mission-critical operations, and the need to maintain comprehensive visibility across extended lifecycles. The solutions that proved effective for managing combat aircraft development and spacecraft missions may prove equally effective for managing enterprise information systems and security postures. Measured evidence supports this potential: \citeauthor{Rogers_Mitchell_2021} documented an eighteen percent improvement in systems engineering efficiency and a nine percent reduction in defects following MBSE adoption on a complex system-of-systems program~\cite{Rogers_Mitchell_2021}. Yet such measured evidence remains rare, and no comparable data exist for enterprise IT or Information Assurance contexts.

Digital Engineering rests upon four foundational pillars: Model-Based Systems Engineering (MBSE), digital threads (the authoritative traceability that connects system lifecycle artifacts), digital twin technology, and Product Lifecycle Management (PLM). Understanding these pillars provides context for examining how Digital Engineering practices might address the challenges identified in current information assurance and IT service management practice.

The integration among these pillars distinguishes Digital Engineering from isolated tool adoption. Organizations might implement modeling tools without achieving Model-Based Systems Engineering, or deploy digital twin capabilities without establishing digital thread traceability. Digital Engineering's value emerges when these capabilities function together as an integrated approach---precisely the integration that current information assurance and IT service management practices lack.

\subsection{Model-Based Systems Engineering}

Model-Based Systems Engineering represents a paradigm shift from document-centric practices to model-centric approaches for system development and management~\cite{Hutchison_2020}. Rather than relying primarily upon textual descriptions and static diagrams, MBSE employs formal, executable models that capture system architecture, behavior, requirements, and relationships in structured, machine-readable formats. These models serve as authoritative sources of truth that can be analyzed, simulated, and validated throughout the system lifecycle~\cite{Hutchinson_2021}.

The distinction between document-centric and model-centric approaches warrants emphasis because it defines the fundamental transformation that Digital Engineering proposes. Document-centric approaches produce artifacts---diagrams, specifications, procedures---that describe systems. These artifacts require human interpretation, cannot be automatically validated for consistency, and provide no mechanisms for maintaining currency as systems evolve. When an organization produces a network diagram, that diagram captures system state at the moment of creation. Every subsequent change renders the diagram incrementally less accurate. When an organization writes a security plan, that plan describes intended security posture at the time of writing. Every subsequent system modification creates potential divergence between the plan and operational reality. Model-centric approaches produce executable representations that can be queried, analyzed, and validated automatically. When models change, dependent artifacts update automatically. When proposed changes are evaluated, models enable impact analysis that documents cannot provide. This automated consistency maintenance addresses the fundamental velocity mismatch between documentation processes and operational change rates that undermines current information assurance and IT service management practices.

The formal modeling languages that underpin MBSE implementations provide the technical foundation for these capabilities. The Systems Modeling Language (SysML), standardized through the Object Management Group, extends the Unified Modeling Language (UML) with constructs specifically designed for systems engineering~\cite{Friedenthal_Moore_Steiner_2014}. Where UML focuses upon software system modeling, SysML adds requirements modeling, parametric analysis, and physical system representation that UML lacks. SysML defines nine diagram types organized into four pillars: requirements diagrams for capturing and tracing stakeholder requirements; structure diagrams including block definition diagrams and internal block diagrams for representing system architecture, composition, and interconnections; behavior diagrams including activity diagrams, sequence diagrams, and state machine diagrams for modeling system dynamics and operational processes; and parametric diagrams for expressing constraint relationships and enabling quantitative analysis. This diagram taxonomy provides a comprehensive vocabulary for representing complex systems from multiple perspectives simultaneously---a capability that directly supports the multi-stakeholder visibility requirements of both information assurance governance and IT service management coordination.

The evolution from SysML 1.x to the emerging SysML v2 specification addresses limitations that impeded broader adoption. SysML v2 introduces a redesigned language architecture emphasizing improved precision, expressiveness, and usability. The new specification provides a standardized textual notation alongside the graphical notation, enabling model creation through text-based interfaces more familiar to software developers and IT professionals. SysML v2 also introduces a standardized Application Programming Interface (API) for tool interoperability, addressing a persistent criticism that MBSE tool ecosystems require vendor-specific integrations. This improved interoperability reduces the vendor lock-in concerns that organizations evaluating MBSE adoption frequently cite. For enterprise IT contexts, the textual notation and API standardization lower adoption barriers by aligning MBSE practices more closely with the text-based configuration and automation workflows that IT professionals employ daily.

The MBSE tool ecosystem encompasses both commercial and open source platforms. Commercial tools---including Dassault Syst√®mes Cameo Systems Modeler and IBM Engineering Systems Design Rhapsody---provide comprehensive modeling environments with enterprise support, training resources, and integration capabilities developed through decades of aerospace and defense application. Open source alternatives have emerged primarily through the Eclipse Foundation's modeling ecosystem. Eclipse Papyrus provides an open source UML and SysML modeling environment~\cite{Eclipse_Papyrus_2024}. Capella, developed by Thales and contributed to Eclipse, provides a comprehensive MBSE tool implementing the Arcadia methodology, achieving significant industrial adoption across aerospace, energy, and transportation sectors~\cite{Capella_MBSE_2024}. SysON, currently under development, implements the SysML v2 specification with a modern web-based architecture designed for collaborative modeling~\cite{SysON_2025}. These open source tools reduce economic barriers to MBSE adoption. However, their documentation, community resources, and application examples focus predominantly upon traditional systems engineering domains. Organizations seeking to apply MBSE tools for enterprise IT or Information Assurance purposes must adapt without domain-specific guidance---an adaptation that remains unexplored in the academic literature.

Architecture frameworks provide the structural foundation for organizing MBSE implementations. Within MBSE practice, architecture frameworks define the viewpoints, views, and model elements that architects employ to represent systems. The Unified Architecture Framework, discussed in detail in a subsequent subsection, provides the most comprehensive architecture framework for MBSE implementations spanning both defense and commercial domains. The multi-viewpoint approach inherent in architecture frameworks aligns naturally with the needs of organizations managing information systems that must satisfy both information assurance requirements and IT service delivery objectives, enabling representation of security controls, operational processes, service dependencies, and resource allocations within a single integrated model environment.

Within the context of information systems, MBSE principles enable organizations to create formal models of their IT infrastructure, security architectures, and service delivery processes. These models capture not merely the static configuration of systems but also the dynamic relationships between components, the flow of information through the enterprise, and the dependencies that affect both security postures and service delivery. Model-based approaches provide enhanced visibility into system complexity, enable automated analysis of security implications for proposed changes, and support more effective planning for IT service delivery requirements. The integration of MBSE with established frameworks such as the NIST RMF and ITIL enables organizations to maintain living models that reflect both security control implementations and service configuration states. However, achieving this integration requires modeling language extensions and domain-specific profiles that have not yet been developed or validated for enterprise IT contexts. SysML provides excellent support for modeling physical systems with requirements, behaviors, and structures, but modeling IT services, network configurations, and security controls requires adaptation or extension that practitioners must develop independently. The absence of standardized approaches for modeling enterprise IT in SysML constitutes a technical barrier to adoption that accompanies the awareness and perception barriers this research investigates.

\subsection{Digital Threads}

Digital threads constitute authoritative traceability---the verified, bidirectional connections between requirements, design elements, implementation artifacts, and validation activities that persist throughout a system's lifecycle~\cite{baker2020system}. The term describes the connective tissue that weaves together authoritative sources including Model-Based Systems Engineering models, requirements management systems, configuration management databases, and Product Lifecycle Management repositories into a unified, navigable fabric of system information. Digital threads ensure that organizations can track how requirements flow through the development and implementation process, identify which system components implement specific capabilities, and verify that implemented solutions satisfy intended requirements. Unlike traditional documentation approaches, digital threads maintain verified relationships that remain current as systems evolve~\cite{zhang2021architecture}.

The concept of authoritative traceability deserves careful attention because it addresses a fundamental limitation of current information assurance and IT service management practice. Traditional traceability attempts to maintain connections through manual cross-references, requirements matrices, and documentation linkages. These manual traceability mechanisms require constant maintenance, degrade as systems evolve, and provide no automated verification of consistency. A requirements traceability matrix that maps security controls to implementation artifacts represents a snapshot of intended relationships at the time of creation. Every subsequent change to either controls or implementations creates potential inconsistency that manual processes may not detect. Digital threads establish traceability through model relationships that update automatically as models change. Queries against digital thread repositories return current rather than historical information. Impact analyses traverse digital thread connections to identify affected components throughout the system architecture. This automated currency maintenance distinguishes digital threads from the manual traceability approaches that current compliance practices employ.

The Department of Defense Digital Engineering Strategy positions the digital thread as essential infrastructure for achieving the authoritative source of truth---the single, trusted baseline of system information from which all stakeholders operate~\cite{DoD_DE_Strategy_2018}. The authoritative source of truth concept addresses a persistent challenge in both defense acquisition and enterprise IT management: when multiple documentation artifacts describe the same system, discrepancies inevitably emerge, and no definitive mechanism exists for determining which artifact accurately reflects operational reality. Digital threads resolve this challenge by establishing verified connections between all system artifacts, ensuring that changes propagate consistently and that queries return authoritative rather than potentially obsolete information. The Systems Engineering Research Center has produced foundational research supporting digital thread implementation, including the Enterprise System-of-Systems Model for Digital Thread Enabled Acquisition, which establishes architectural patterns for maintaining traceability across complex, multi-system environments~\cite{SERC_Digital_Thread_2018}. DoDI 5000.97 codifies these concepts into mandatory requirements, directing defense programs to maintain digital thread capabilities throughout the acquisition lifecycle~\cite{DoDI_5000_97_2023}.

For information assurance practice, digital threads address gaps in current RMF implementation. The RMF requires organizations to select security controls, implement those controls, and assess their effectiveness throughout the system lifecycle. Digital threads enable organizations to trace security requirements from categorization decisions through control selection, implementation, and assessment activities---connecting policy documents to technical configurations to assessment evidence in a single authoritative chain. This traceability supports the continuous monitoring phase of the RMF by maintaining verifiable connections between security requirements, implemented controls, and compliance artifacts. When a security control implementation changes, digital thread connections enable automated identification of affected requirements, dependent controls, and compliance documentation requiring update. When auditors require evidence of control implementation, digital threads provide navigable paths from requirements through implementation to assessment results without requiring manual evidence compilation.

Within IT service management contexts, digital threads align with ITIL configuration management and change management practices. Organizations can trace service delivery requirements to underlying infrastructure components and configuration items, connecting the CMDB to as-built system documentation and operational baselines. This capability supports more accurate impact assessment for changes and more effective root cause analysis for service disruptions. When a proposed change affects a configuration item, digital thread connections reveal which services depend upon that item, which security controls protect it, and which compliance requirements govern it. The ability to maintain current, verified traceability relationships through digital threads reduces the time and effort required for compliance audits while improving the accuracy of both security assessments and service impact analyses.

The practical implementation of digital threads requires integration infrastructure that connects diverse tools and repositories. In defense contexts, this integration has developed over years of investment and standardization. Enterprise IT environments employ different tool ecosystems---IT service management platforms like ServiceNow and BMC, security information and event management systems, cloud management consoles, and configuration automation tools---that were not designed with digital thread connectivity in mind. Establishing digital thread capabilities in enterprise IT contexts therefore requires integration approaches that bridge systems engineering tools and IT management platforms. This integration challenge represents a practical barrier that accompanies the conceptual transfer of digital thread principles from defense to enterprise IT contexts.

\subsection{Digital Twin Technology}

Digital twin technology creates virtual replicas of physical or logical systems that maintain synchronization with their real-world counterparts through continuous data exchange~\cite{Shao_2021}. These virtual representations enable organizations to simulate system behavior, analyze potential changes, predict future states, and optimize performance without disrupting operational systems~\cite{madni2018leveraging}. Digital twins combine real-time operational data with analytical models to provide dynamic, predictive capabilities that extend far beyond traditional monitoring and simulation approaches~\cite{Khan_Saad_Niyato_Han_Hong_2022}.

The digital twin concept originated in Product Lifecycle Management research. \citeauthor{Grieves_2023} traces the evolution of digital twins from their conceptual origins through contemporary applications, positioning digital twins as the integration of physical and virtual spaces that enables analysis, optimization, and prediction across system lifecycles~\cite{Grieves_2023}. The foundational distinction between digital twins and traditional simulation lies in synchronization: where traditional simulations model hypothetical or designed system states, digital twins maintain continuous data exchange with their physical or logical counterparts, enabling operational decision-making based upon current rather than documented system states. This synchronization characteristic directly addresses the documentation-reality gap that undermines information assurance and IT service management practices. Static models represent intended or designed system states; digital twins represent current operational states, updated continuously through integration with operational data sources. This distinction carries profound implications for organizations that must make security and operational decisions based upon accurate understanding of system configurations.

Standardization efforts have established frameworks for digital twin implementation that reduce interoperability concerns and provide structured guidance. The International Organization for Standardization published ISO 23247, the Digital Twin Framework for Manufacturing, defining a four-part reference architecture encompassing observable manufacturing elements, digital twin entities, data collection and device control infrastructure, and cross-domain integration~\cite{ISO_23247_2021}. \citeauthor{Shao_ISO_23247_2023} provided detailed analysis of ISO 23247's structure and applicability~\cite{Shao_ISO_23247_2023}. While developed for manufacturing contexts, the reference architecture establishes patterns---data synchronization, entity representation, cross-domain integration---that inform digital twin implementations in other domains. The IEC 62832 standard complements ISO 23247 by addressing industrial-process measurement, control, and automation aspects of digital twin frameworks~\cite{Shao_2021}. The Internet Engineering Task Force has published a draft reference architecture specifically addressing network infrastructure digital twins, representing direct engagement with the IT infrastructure domain~\cite{IETF_NDT_2024}. This IETF engagement signals that the networking community has recognized digital twin applicability for IT infrastructure, though the published architecture remains at draft stage and has not yet achieved the maturity of manufacturing-focused standards.

The National Institute of Standards and Technology has engaged with digital twin technology through NIST Internal Report 8356, which addresses cybersecurity challenges and trust considerations for digital twin implementations~\cite{NIST_IR_8356_2025}. This publication examines security considerations \textit{for} systems employing digital twins rather than digital twin applications \textit{to} Information Assurance---a distinction that reveals the current orientation of institutional research. NIST's Framework for Cyber-Physical Systems similarly addresses digital integration challenges at the intersection of physical and computational systems~\cite{NIST_CPS_Framework_2017}. Together, these publications establish that authoritative institutions recognize digital twin relevance to security domains while treating digital twins primarily as systems requiring security rather than as tools for enhancing security operations. The reorientation from securing digital twins to employing digital twins for security purposes represents a conceptual shift that the academic literature has begun but not yet completed.

Open source frameworks have emerged to reduce economic and technical barriers to digital twin adoption. The Digital Twin Consortium has established an open source collaboration initiative providing frameworks and reference implementations~\cite{DTC_OpenSource_2024}. Eclipse Ditto, maintained by the Eclipse Foundation, provides an open source framework for creating and managing digital twins in Internet of Things applications~\cite{Eclipse_Ditto_2024}. Eclipse BaSyx implements the Asset Administration Shell standard for industrial digital twins~\cite{Eclipse_BaSyx_2024}. \citeauthor{Gil_2024} conducted a systematic survey of fourteen open source digital twin frameworks, evaluating them against criteria derived from ISO 23247 standards and finding significant variation in maturity, documentation quality, and community support~\cite{Gil_2024}. \citeauthor{Autiosalo_Siegel_Tammi_2021} introduced Twinbase, an open source server for the Digital Twin Web concept, enabling organizations to publish and discover digital twin descriptions through standardized interfaces~\cite{Autiosalo_Siegel_Tammi_2021}. These open source options provide accessible entry points for organizations exploring digital twin capabilities, though their documentation and application examples focus upon manufacturing and IoT rather than enterprise IT or Information Assurance contexts.

In information assurance contexts, digital twin capabilities offer advantages for security control validation, risk assessment, and incident response. Organizations can create digital twins of their information systems to simulate security scenarios, test control effectiveness, and analyze attack vectors in environments isolated from production operations. These virtual representations enable security teams to evaluate the impact of proposed security controls, assess the effectiveness of defensive measures, and predict system behavior under various threat scenarios. Digital twins support RMF assessment activities by enabling organizations to test security configurations and validate control implementations before deployment to production environments. The ability to simulate adversary behavior against synchronized representations of actual system configurations enables security testing that reflects operational reality rather than documented assumptions.

For IT service delivery, digital twins support capabilities aligned with ITIL service design and service transition practices. Organizations can employ digital twins for capacity planning, change impact analysis, and service optimization by enabling teams to test changes and analyze performance implications before implementing modifications in production environments. The ability to simulate proposed changes in a synchronized virtual environment reduces the risk of service disruptions while supporting more rapid and confident change implementation. When combined with digital thread traceability, digital twins enable organizations to assess the security and operational implications of proposed changes simultaneously---an integrated analysis capability that current practices, which typically assess security and operational impacts through separate processes, cannot provide.

\subsection{Product Lifecycle Management}

Product Lifecycle Management provides frameworks and toolsets for managing information, processes, and resources throughout a system's entire lifecycle from initial conception through retirement~\cite{Ross_Winstead_McEvilley_2022}. PLM integrates data from diverse sources, maintains configuration baselines, manages change processes, and ensures that stakeholders access current, accurate information about system states and changes. This integrated approach to lifecycle management extends beyond simple version control to encompass configuration management, change coordination, release management, and information governance.

PLM originated in manufacturing and product development, where organizations confronted the challenge of managing complex products involving thousands of components, multiple suppliers, extended development timelines, and rigorous quality requirements. The manufacturing sector developed PLM practices to address challenges that bear structural similarity to those confronting enterprise IT management: maintaining accurate documentation of complex configurations, coordinating changes across interdependent components, ensuring consistency between design documentation and as-built products, and managing compliance with regulatory requirements throughout product lifecycles. The commercial PLM ecosystem has matured through decades of industrial application, producing platforms that manage millions of engineering artifacts across global supply chains. Open source PLM alternatives have emerged to reduce cost barriers, though \citeauthor{Laili_2024} noted that industrial open source PLM solutions vary considerably in maturity and require careful evaluation against organizational requirements~\cite{Laili_2024}.

The application of PLM principles to information systems represents a conceptual extension from these manufacturing origins. Physical products have lifecycles that parallel information system lifecycles in important ways: conception, design, development, deployment, operation, maintenance, and retirement. PLM practices developed for managing physical product lifecycles address challenges---configuration management, change coordination, baseline maintenance---that information system managers confront daily. The question is whether PLM tools and methodologies can be adapted effectively for information system contexts, where system components are logical rather than physical, change cycles occur in hours rather than months, and deployment involves software configurations rather than manufacturing processes.

Applied to information systems, PLM principles address challenges in managing complex IT infrastructures and security postures throughout the system lifecycle. The RMF explicitly recognizes the importance of lifecycle management, requiring organizations to maintain security controls and documentation throughout system operation and into decommissioning. PLM approaches support these requirements by managing security control baselines, coordinating changes across interconnected systems, maintaining configuration integrity, and ensuring that security teams operate from consistent, current information throughout the authorization boundary. PLM configuration baseline capabilities directly address the requirement in NIST SP 800-53 control CM-2 for documented, maintained system baselines~\cite{Force_2020}. PLM change coordination capabilities support control CM-3 requirements for configuration change control. PLM access governance capabilities align with CM-5 requirements for access restrictions governing system changes. These correspondences between PLM capabilities and specific compliance requirements suggest that PLM practices could address compliance obligations that organizations currently struggle to fulfill through manual processes.

PLM capabilities align closely with ITIL service lifecycle management concepts. Organizations can implement PLM frameworks to support ITIL configuration management by maintaining authoritative configuration baselines and managing configuration item relationships. PLM change coordination capabilities enhance ITIL change management by providing improved visibility into change impacts across service dependencies. The ability to maintain integrated views of system configurations, security controls, and service delivery components reduces inconsistencies between security and IT operations teams, improves change coordination, and supports more effective compliance management across the system lifecycle. PLM's emphasis upon managing information across organizational boundaries addresses the silo problem that fragments visibility in current IT environments: security teams, operations teams, and application teams can operate from a shared, authoritative information base rather than maintaining separate, potentially inconsistent documentation.

\subsection{Institutional Endorsement and Strategic Direction}

Digital Engineering's maturation from experimental practice to institutional mandate provides important context for evaluating its potential applicability beyond defense and aerospace. The breadth and depth of institutional endorsement establishes that Digital Engineering represents not an experimental approach advocated by enthusiasts but a mature discipline validated by the organizations most experienced in managing complex, compliance-intensive systems. Understanding this institutional foundation informs assessment of whether Digital Engineering merits investigation for enterprise IT and Information Assurance applications.

The Department of Defense Digital Engineering Strategy, published in June 2018, established the formal vision for transforming defense acquisition through Digital Engineering practices~\cite{DoD_DE_Strategy_2018}. The strategy defines five strategic goals that collectively articulate the transformation Digital Engineering envisions: formalize the development, integration, and use of models to inform enterprise and program decision-making; provide an authoritative source of truth by establishing a digital thread as the means of delivering the technical baseline; incorporate technological innovation to improve the engineering practice; establish a Digital Engineering ecosystem to provide the infrastructure and environments for performing Digital Engineering activities; and transform the culture and workforce to adopt and support Digital Engineering across the lifecycle. These goals extend beyond technology adoption to encompass organizational transformation, workforce development, and cultural change---dimensions that any transfer of Digital Engineering to enterprise IT contexts must address.

DoD Instruction 5000.97, issued in December 2023, codified these strategic aspirations into mandatory requirements for defense programs~\cite{DoDI_5000_97_2023}. The instruction mandates that programs leverage digital artifacts as the authoritative source of system information and maintain digital thread capabilities throughout the acquisition lifecycle. This transition from strategic vision to regulatory mandate represents a significant institutional commitment: defense programs must now implement Digital Engineering practices rather than merely consider them. The Systems Engineering Guidebook provides detailed implementation guidance for meeting these requirements, establishing processes, methods, and tool expectations for Digital Engineering practice within defense acquisition~\cite{DoD_Engineering_2022}. The progression from strategy document to formal instruction to implementation guidebook demonstrates institutional maturation that parallels the compliance framework progression familiar in information assurance contexts---from policy through standards through implementation guidance.

NASA has pursued a parallel path toward Digital Engineering adoption, lending independent validation to the approach. The NASA Digital Engineering Acquisition Framework Handbook, NASA-HDBK-1004, establishes guidance for incorporating Digital Engineering practices into NASA programs~\cite{NASA_HDBK_1004_2020}. The NASA Future Model-Based Systems Engineering Vision and Strategy Bridge document articulates NASA's long-term vision for MBSE adoption across the agency's portfolio of complex, mission-critical programs~\cite{NASA_MBSE_Vision_2021}. NASA's independent development of Digital Engineering guidance carries particular significance: both the Department of Defense and NASA confronted similar challenges---complex systems demanding comprehensive documentation, stringent compliance requirements, mission-critical operations tolerating no ambiguity in system understanding---and both converged upon Digital Engineering as the solution. This independent convergence strengthens the argument that Digital Engineering addresses fundamental challenges in managing complex, compliance-intensive systems rather than representing a domain-specific solution applicable only to defense acquisition.

The International Council on Systems Engineering has positioned Digital Engineering as the future direction of the systems engineering discipline. The INCOSE Systems Engineering Vision 2035 establishes a roadmap for transforming systems engineering practice through Digital Engineering, model-based methods, and integrated digital environments~\cite{INCOSE_Vision_2035}. The INCOSE Systems Engineering Handbook, Fifth Edition, provides comprehensive guidance for implementing systems engineering practices including MBSE, reflecting the discipline's current state of practice and serving as the primary professional reference for systems engineers worldwide~\cite{INCOSE_Handbook_2023}. INCOSE's Digital Engineering Information Exchange Working Group specifically addresses the interoperability and integration challenges that arise when organizations attempt to exchange digital engineering artifacts across organizational and tool boundaries~\cite{DigitalEngineeringInformationExchange}. This working group's existence acknowledges that Digital Engineering implementation requires addressing practical interoperability challenges---challenges that would intensify when extending Digital Engineering beyond its established systems engineering context into enterprise IT domains.

The Systems Engineering Body of Knowledge (SEBoK), jointly managed by INCOSE, the IEEE Systems Council, and the Stevens Institute Systems Engineering Research Center, provides the globally recognized authoritative reference defining Digital Engineering's relationship to MBSE, digital threads, and authoritative sources of truth within the ISO/IEC/IEEE 15288:2023 systems engineering lifecycle framework~\cite{SEBoK_2024}. SEBoK establishes the conceptual vocabulary and architectural relationships that the systems engineering community employs when discussing Digital Engineering, providing the definitional foundation upon which this dissertation's investigation builds.

The Systems Engineering Research Center (SERC) has produced foundational research publications supporting Digital Engineering implementation and assessment. The Digital Engineering Competency Framework defines 962 Knowledge, Skills, Abilities, and Behaviors organized by proficiency levels, providing the most comprehensive specification of what professionals require to practice Digital Engineering effectively~\cite{Hutchinson_2021}. The Digital Engineering Metrics framework establishes measurement approaches for assessing adoption progress and benefits realization, addressing the measurement challenge that the literature identifies as a persistent barrier to evidence-based adoption decisions~\cite{Hutchison_2020}. The SERC Systems Engineering Modernization report examines the integration of Digital Engineering with Mission Engineering, Agile, and DevOps practices, establishing connections between Digital Engineering and the development methodologies prevalent in enterprise IT contexts~\cite{SERC_SE_Modernization_2022}. These SERC publications provide the research infrastructure---competency definitions, metrics, and integration frameworks---that would support Digital Engineering adoption in new domains, including enterprise IT and Information Assurance.

NIST's contributions span multiple relevant domains. The Framework for Cyber-Physical Systems addresses integration challenges at the intersection of physical and computational systems~\cite{NIST_CPS_Framework_2017}. NIST's systems security engineering publications---Special Publication 800-160 Volume 1 Revision 1 on Engineering Trustworthy Secure Systems~\cite{Ross_Winstead_McEvilley_2022} and Volume 2 Revision 1 on Developing Cyber-Resilient Systems~\cite{NIST_SP_800_160_V2_2021}---reference model-based approaches and traceability requirements that align with Digital Engineering principles. NIST Internal Report 8356 on digital twin security considerations~\cite{NIST_IR_8356_2025} and the Open Security Controls Assessment Language initiative~\cite{NIST_OSCAL_2023} further demonstrate NIST engagement with concepts that Digital Engineering could integrate. These publications establish that the institution responsible for federal cybersecurity standards recognizes the relevance of model-based approaches, traceability, and digital integration to security engineering---even though NIST has not yet explicitly connected these concepts under the Digital Engineering framework or provided specific guidance for their application in enterprise IT contexts. The gap between NIST's recognition of relevant principles and the absence of specific Digital Engineering implementation guidance for enterprise IT constitutes one of the research gaps this dissertation addresses.

\subsection{Enterprise Architecture Frameworks}

Enterprise architecture frameworks provide the structural foundation upon which Digital Engineering implementations rest. These frameworks define how organizations represent, analyze, and manage the complex relationships among business processes, information flows, application systems, and technology infrastructure. Understanding the enterprise architecture framework ecosystem is essential for evaluating Digital Engineering's potential in enterprise IT contexts because the frameworks determine what can be modeled, how models are organized, and what analytical capabilities models support.

The Unified Architecture Framework, now codified as ISO/IEC 19540 through the Object Management Group~\cite{OMG_UAF_2022}, represents the most significant evolution in enterprise architecture standardization. UAF emerged from the consolidation of military architecture frameworks---the Department of Defense Architecture Framework (DoDAF)~\cite{DoDAF_2009}, the UK Ministry of Defence Architecture Framework (MODAF), and the NATO Architecture Framework~\cite{NATO_NAF_2018}---with the explicit recognition that the architectural concepts developed for military system-of-systems management possess broad applicability beyond defense contexts. The Object Management Group determined that approximately ninety percent of concepts captured in military architecture frameworks prove equally applicable in commercial domains~\cite{OMG_UAF_About_2024}, motivating the development of a unified framework that serves both defense and commercial organizations without requiring domain-specific adaptations.

UAF employs a grid-based structure defining seventy-one view specifications organized through two complementary specifications: the UAF Domain Metamodel (DMM), which defines the underlying concepts, relationships, and constraints; and the UAF Modeling Language (ML), which provides the concrete notation for expressing architecture descriptions~\cite{OMG_UAF_Spec_2022}. The grid rows represent stakeholder domains---Strategic, Operational, Services, Personnel, and Resources---while columns represent architecture aspects including taxonomy, structure, connectivity, processes, states, interaction, constraints, and traceability. This grid organization enables architects to represent any complex system-of-systems from multiple stakeholder perspectives simultaneously, with formal relationships connecting views across the grid to maintain consistency. The traceability column explicitly addresses the requirement for maintaining verified connections across architectural perspectives---a requirement that aligns directly with the digital thread concept and with the compliance traceability obligations that NIST SP 800-53 imposes.

The evolution from domain-specific frameworks to the unified standard proceeded through several stages. DoDAF 2.0 established the foundational metamodel for defense architecture description~\cite{DoDAF_2009}. MODAF independently developed architecture description conventions for UK defence applications. The NATO Architecture Framework provided architecture capabilities for alliance-wide interoperability~\cite{NATO_NAF_2018}. Earlier evaluation identified limitations in these individual frameworks: the National Defense Industrial Association Systems Engineering Division found that DoDAF required augmentation to adequately support systems engineering activities~\cite{NDIA_SE_DoDAF_2011}, and research by \citeauthor{Hause_2010} identified specific gaps between DoDAF's metamodel and the requirements of model-based systems engineering practice~\cite{Hause_2010}. The Unified Profile for DoDAF and MODAF (UPDM) represented an initial consolidation effort that eventually evolved into UAF under broader OMG stewardship, with ISO/IEC international standardization following to establish UAF as a globally recognized rather than organizationally specific standard~\cite{OMG_UAF_UPDM_2017}.

Comparative research validates UAF's position relative to alternative enterprise architecture frameworks. \citeauthor{Bankauskaite_2019} evaluated enterprise architecture frameworks using weighted criteria encompassing completeness, maturity, standardization, and tool support, finding that UAF achieved the highest overall rating among the frameworks evaluated, surpassing TOGAF~\cite{TOGAF_2018}, DoDAF, MODAF, NAF, and FEAF~\cite{Bankauskaite_2019}. This comparative advantage derives from UAF's comprehensive metamodel, its ISO international standardization, and its explicit design for both defense and commercial applications. Research by \citeauthor{Eichmann_Melzer_God_2019} demonstrated UAF application to system-of-systems development~\cite{Eichmann_Melzer_God_2019}. \citeauthor{Liu_2023} applied UAF to military system-of-systems design using top-down MBSE methodology~\cite{Liu_2023}. \citeauthor{Torkjazi_2024} extended UAF-based MBSE to integrate autonomy into system-of-systems architectures~\cite{Torkjazi_2024}. \citeauthor{Abhaya_2021} established UAF-Based MBSE methods for building system-of-systems models~\cite{Abhaya_2021}. Collectively, these studies demonstrate UAF's growing adoption and validation across diverse system-of-systems contexts, though applications to enterprise IT infrastructure and Information Assurance remain absent from the published literature.

The complementary relationship between UAF and the commercially prevalent Open Group Architecture Framework (TOGAF) reduces adoption barriers for organizations already invested in enterprise architecture practice. TOGAF provides a comprehensive architecture development method and enterprise architecture governance framework widely adopted in commercial IT contexts~\cite{TOGAF_2018}. A joint white paper between The Open Group and MITRE Corporation established that UAF and TOGAF address different but complementary aspects of enterprise architecture: TOGAF provides the methodology and governance framework for architecture development, while UAF provides the architecture description specification for expressing what has been architected~\cite{OpenGroup_MITRE_2013}. Organizations can retain TOGAF methodology while adopting UAF for architecture description, enabling incremental adoption rather than wholesale replacement of established practices. This compatibility carries particular significance for enterprise IT organizations that have invested in TOGAF-based architecture programs, as it suggests that UAF adoption for model-based architecture description need not require abandonment of existing architecture governance investments.

UAF's foundation upon UML and SysML profiles enables model-based documentation approaches that address the accuracy and currency challenges plaguing traditional enterprise architecture implementations~\cite{Aerospace_UAF_2023}. Because UAF architecture descriptions are expressed as formal models rather than static documents, they inherit the automated consistency checking, impact analysis, and traceability capabilities that model-based approaches provide. This model-based foundation connects enterprise architecture practice to the broader Digital Engineering ecosystem: UAF models can participate in digital threads, maintain synchronization with operational systems through digital twin mechanisms, and be managed through PLM lifecycle processes. The adoption of UAF by the Department of Defense~\cite{DoD_Engineering_2022}, NATO~\cite{NATO_NAF_2018}, and the UK Ministry of Defence, combined with ISO international standardization, establishes UAF as the authoritative consolidating framework for organizations requiring architecture capabilities across multiple domains. For enterprise IT and Information Assurance applications, UAF provides structured approaches to documenting systems, relationships, and security requirements that align with both NIST and ITIL expectations. Understanding this enterprise architecture foundation provides essential context for evaluating how Digital Engineering extends architectural approaches from static documentation toward living, model-based representations that maintain currency with operational reality.

\section{Gaps in Current Practice}

Despite advances in information assurance methodologies and IT service management frameworks, organizations continue to encounter challenges that limit their effectiveness in protecting information assets and delivering reliable services. The NIST Risk Management Framework and ITIL provide structured approaches to information assurance and IT service management respectively. Yet implementation challenges persist across both domains. Digital Engineering practices have demonstrated capabilities in defense and aerospace contexts that address structurally analogous challenges: maintaining authoritative documentation, ensuring configuration visibility, and providing traceability across complex systems~\cite{DoD_DE_Strategy_2018}. Whether these capabilities translate to enterprise IT and information assurance contexts, and whether professionals in those domains recognize the potential relevance, remains uninvestigated. Examining the specific gaps that persist in current practice illuminates the structural parallels that motivate this investigation.

The persistence of these gaps despite framework maturity and organizational investment suggests that the challenges reflect structural limitations rather than implementation failures. Organizations following established frameworks with dedicated resources still experience documentation failures, visibility gaps, and traceability shortfalls. These outcomes indicate that the problem lies not in how organizations execute current approaches but in inherent limitations of document-centric methodologies.

\subsection{Information Assurance Challenges}

Organizations implementing the NIST Risk Management Framework struggle to maintain visibility into their security postures across complex, distributed information systems. The RMF continuous monitoring phase requires organizations to maintain ongoing awareness of security control effectiveness and system security state. Security teams often lack accurate, current understanding of system configurations, security control implementations, and the dependencies that affect security effectiveness. This visibility gap manifests throughout the RMF lifecycle: organizations find it difficult to track security dependencies effectively, leading to unidentified vulnerabilities when changes are implemented. The inability to maintain accurate documentation of system interconnections and data flows, particularly in environments with rapid deployment cycles, impairs the effective risk assessment and incident response capabilities that the RMF demands.

The challenge of understanding cascading impacts deserves particular attention. When security incidents occur or vulnerabilities are discovered, defenders must rapidly assess which systems are affected, what data is at risk, and how compromise of one system might enable access to interconnected systems. This assessment requires understanding of system dependencies that current documentation approaches cannot maintain. The inability to trace first, second, and third order impacts transforms incident response from a precision operation into a broad search effort that consumes time and resources while adversaries retain the initiative.

Security control implementation presents particular challenges in modern enterprise environments characterized by hybrid cloud deployments, distributed architectures, and frequent changes. Organizations must implement and maintain consistent security controls from NIST SP 800-53 across diverse platforms while supporting continuous deployment practices and rapid update cycles~\cite{Force_2020}. Traditional security configuration management approaches fail to scale effectively in these dynamic environments, leading to inconsistent security postures and compliance gaps. The challenge compounds when organizations attempt to validate control effectiveness across interconnected systems where authorization boundaries grow increasingly complex to define and maintain.

\subsection{IT Service Management Challenges}

IT service management faces parallel challenges in maintaining accurate system documentation. Configuration Management Database implementations fail at documented rates approaching eighty percent, leaving organizations without authoritative sources for configuration information~\cite{Gartner_CMDB_2019}. Manual configuration tracking cannot keep pace with the rate of change in modern IT environments. Shadow IT creates blind spots where undocumented systems introduce unknown dependencies and security risks. Change management processes suffer when impact assessments rely upon incomplete or inaccurate dependency information.

The economic dimensions of these failures warrant examination. Organizations invest considerable resources in CMDB implementations, documentation efforts, and change management processes. When these investments fail to deliver intended value, organizations face difficult choices: invest additional resources attempting to improve failing approaches, accept degraded capabilities and increased risk, or seek alternative approaches that address root causes rather than symptoms. Digital Engineering represents a candidate alternative approach whose demonstrated benefits in defense and aerospace contexts suggest potential applicability to enterprise IT environments.

The convergence of these challenges creates a compounding effect where neither information assurance nor IT service management can achieve their objectives independently. Security teams cannot effectively assess risks without accurate understanding of IT infrastructure. IT teams cannot effectively manage changes without understanding security implications. Both domains require the visibility and accurate documentation that current practices demonstrably fail to provide.

\subsection{The Documentation-Reality Gap}

The persistent gap between documentation and operational reality represents the common thread connecting failures across both domains. Security documentation describes control implementations that may not exist as documented. Configuration databases contain information that no longer reflects system states. Network diagrams depict architectures that have evolved beyond their documented form. This gap undermines every process that depends upon accurate system information.

When documentation diverges from reality, security assessments measure fiction rather than fact. Change impact analyses miss dependencies that exist but are not documented. Incident responders waste time discovering that documented configurations do not match operational systems. Compliance auditors cannot verify that documented controls exist in practice. The documentation-reality gap transforms information assurance and IT service management from disciplined practices into exercises in uncertainty.

Digital Engineering offers capabilities designed to address this gap through its emphasis upon authoritative sources of truth, continuous synchronization between models and operational systems, and automated verification of consistency between documentation and reality. Defense and aerospace organizations have demonstrated these capabilities in complex, compliance-intensive environments~\cite{DoD_DE_Strategy_2018}. The question this research investigates is whether IT and information assurance professionals recognize the potential value of these capabilities for their work---a prerequisite question for determining whether Digital Engineering practices developed in defense contexts merit investigation for broader enterprise application.

\section{Research Questions}

Based upon the challenges documented in current practice and the potential capabilities offered by Digital Engineering, this research investigates the following questions:

\begin{enumerate}
  \item To what extent are information technology and information assurance professionals aware of Digital Engineering capabilities, including Model-Based Systems Engineering, digital threads, digital twin technologies, and Product Lifecycle Management principles?

  \item Do information technology and information assurance professionals perceive Digital Engineering capabilities as potentially valuable or important for their work in information assurance, security compliance, and IT service delivery?

  \item Do information technology and information assurance professionals believe that Digital Engineering practices could help them in performing their jobs, meeting compliance requirements, or enhancing organizational capabilities in information assurance and IT service delivery?
\end{enumerate}

These research questions focus upon professional awareness and perceptions as foundational investigation. Establishing awareness levels and perceived value represents an essential first step before investigating practical implementation approaches, organizational adoption strategies, or empirical validation of Digital Engineering benefits in information assurance and IT service management contexts. Preliminary evidence from adjacent domains suggests that such investigation is urgently needed: \citeauthor{Henderson_McDermott_Salado_2024} found that approximately twenty-two percent of systems engineering professionals---practitioners within the discipline that developed MBSE---could not articulate a clear definition of MBSE~\cite{Henderson_McDermott_Salado_2024}. If awareness deficits persist within the originating discipline, awareness among professionals in Information Assurance and IT Service Management---disciplines that have never systematically encountered these methodologies---requires empirical measurement rather than assumption.

The distinction between Research Question 2 and Research Question 3 warrants explicit clarification, as both address professional perceptions of Digital Engineering value. Research Question 2 measures whether professionals recognize Digital Engineering capabilities as abstractly valuable or important for their professional domains---an assessment of general relevance that does not require the respondent to evaluate specific operational impact within their own work context. Research Question 3 probes a more personal and practical construct: whether professionals believe Digital Engineering could tangibly help them perform their jobs, meet the compliance requirements they personally face, or enhance the specific organizational capabilities they are responsible for delivering. Technology adoption literature distinguishes between recognizing that a capability has general value and believing that the same capability would improve one's own work. Professionals may acknowledge that Digital Engineering offers theoretical value for their domain while remaining skeptical about its practical applicability to their specific organizational context. This distinction carries significant implications for adoption strategy: a professional community that perceives general value but doubts personal applicability requires different intervention than one that perceives both.

\section{Research Scope and Approach}

This research examines professional perceptions across several key areas. The investigation focuses upon awareness and perceived value of Model-Based Systems Engineering approaches for representing information system architectures and security controls. It examines whether professionals perceive value in digital threads for maintaining authoritative traceability between security requirements, control implementations, and compliance evidence as required by frameworks such as the NIST RMF\@. The research explores perceptions of digital twin capabilities for security simulation, testing, and IT service modeling. And it investigates whether professionals perceive value in Product Lifecycle Management principles for managing information system configurations and security control baselines throughout the system lifecycle.

\subsection{Methodological Approach}

The research employs a quantitative survey methodology to collect data from IT and information assurance professionals. Survey methodology enables systematic data collection from a broad population of practitioners, supporting statistical analysis and generalization of findings. The anonymous nature of survey research encourages candid responses about professional knowledge gaps and organizational capabilities. Chapter 3 presents the complete research methodology including survey design, sampling strategy, and analytical approach.

\subsection{Target Population}

The research targets IT and information assurance professionals across diverse organizational contexts. This population includes security analysts, compliance officers, IT managers, network administrators, systems administrators, and other professionals who implement and maintain information assurance controls or deliver IT services. By surveying professionals across organizational contexts rather than limiting investigation to a single industry or organization, the research enables assessment of awareness and perceived value across the professional community.

The target population intentionally extends beyond the systems engineering professionals who have been the focus of previous MBSE adoption research. While studies by \citeauthor{Call_2024} and \citeauthor{Henderson_McDermott_Salado_2024} examined perceptions among systems engineering professionals~\cite{Call_2024, Henderson_McDermott_Salado_2024}, no comparable research addresses IT and Information Assurance practitioners. This population represents a crucial test of Digital Engineering's transferability: if professionals who have never encountered these methodologies nevertheless recognize their potential value for addressing familiar challenges, the case for cross-disciplinary investigation strengthens substantially.

\subsection{Potential Benefits for Organizations Serving Underrepresented Populations}

The potential transferability of Digital Engineering benefits carries particular significance for organizations serving underrepresented and underserved populations. Healthcare providers serving rural communities, educational institutions in under-resourced districts, social service organizations with limited IT budgets, and non-profit entities addressing community needs all require effective information assurance and IT service delivery capabilities. These organizations face the same documentation challenges, visibility gaps, and compliance burdens as large enterprises, often with fewer resources to address them.

Organizations serving underrepresented populations must frequently demonstrate compliance with regulatory frameworks, security standards, and funding requirements. Healthcare providers must satisfy HIPAA security requirements. Educational institutions must protect student data under FERPA. Social service organizations must safeguard client information while demonstrating accountability to funding agencies. These compliance obligations demand documented evidence of security controls and system configurations---documentation that consumes scarce staff time and organizational resources.

If Digital Engineering practices can reduce the burden of compliance documentation while improving documentation accuracy, organizations with limited resources could redirect staff effort toward direct service provision rather than documentation administration. The automated traceability that digital threads provide could reduce the manual effort required for compliance audits. The model-based documentation that MBSE enables could maintain accuracy through automated synchronization rather than manual updates. The configuration management capabilities that PLM provides could reduce the specialized expertise required to maintain accurate system documentation.

This research does not presume that Digital Engineering benefits will transfer effectively to resource-constrained organizations. The survey population targets IT and information assurance professionals broadly, not exclusively those serving underrepresented populations. However, by establishing baseline awareness and perceived value data across the professional community, this research creates a foundation for subsequent investigation of Digital Engineering applicability in diverse organizational contexts. If professionals perceive value in Digital Engineering capabilities, future research can examine practical implementation approaches suitable for organizations with varying resource levels.

The logical pathway from defense and aerospace demonstration to broader application proceeds through several steps. First, defense and aerospace organizations demonstrate that Digital Engineering delivers measurable benefits for complex systems with stringent compliance requirements. Second, research establishes whether IT and information assurance professionals outside these sectors recognize potential value in Digital Engineering capabilities. Third, if perceived value exists, subsequent research can examine implementation approaches, adaptation requirements, and cost-benefit considerations for organizations in different contexts. This research addresses the second step: determining whether professional awareness and perceived value support continued investigation of Digital Engineering for enterprise IT and information assurance applications.

\subsection{Why Perceptions Matter}

The investigation of professional perceptions warrants explanation given the availability of alternative research approaches. Technology adoption research consistently demonstrates that perceived value influences adoption decisions regardless of actual value. Professionals who do not perceive value in a capability will not advocate for its adoption within their organizations. Establishing whether IT and information assurance professionals recognize potential value in Digital Engineering capabilities therefore addresses a prerequisite question for successful adoption.

Furthermore, perception research enables assessment of awareness gaps that might impede adoption. If professionals are unaware of Digital Engineering capabilities, education and communication initiatives become necessary precursors to adoption efforts. If professionals are aware but do not perceive value, the theoretical premise that Digital Engineering addresses recognized needs requires reconsideration. Understanding the current state of professional awareness and perceptions enables targeted strategies for advancing Digital Engineering adoption in information assurance and IT service management domains.

\subsection{Contribution of Prior Research}

By surveying professionals actively working in these domains, this research identifies whether practitioners recognize connections between their current practices and Digital Engineering capabilities. The findings illuminate whether existing information assurance and IT service delivery frameworks already incorporate concepts analogous to Model-Based Systems Engineering, digital threads, digital twins, or Product Lifecycle Management, or whether these Digital Engineering capabilities represent genuinely novel approaches within information technology contexts. This understanding establishes whether Digital Engineering offers new conceptual frameworks for addressing information assurance and IT service delivery challenges or whether it primarily provides different terminology for existing practices.

This research builds upon the foundational work of \citeauthor{Bonar_Hastings_2024}, who established an initial reference model demonstrating that compliance verification is enhanced and supported by Digital Engineering practices within the context of information systems~\cite{Bonar_Hastings_2024}. The current research extends this foundation by examining whether the broader professional community recognizes value in the capabilities that the reference model proposes.

\section{Significance of the Research}

This research carries implications across academic, industry, commonwealth, and societal dimensions. Understanding these dimensions of significance contextualizes the contribution this investigation makes to knowledge and practice.

\subsection{Academic Significance}

The academic significance of this research lies in identifying whether IT and information assurance professionals recognize a gap that Digital Engineering may address. The literature review presented in Chapter 2 documents a near-complete absence of academic research applying Model-Based Systems Engineering, digital threads, digital twins, or Product Lifecycle Management to enterprise IT infrastructure or Information Assurance programs. This research gap exists despite explicit requirements within NIST and ITIL frameworks for enterprise architecture capabilities, documentation accuracy, and traceability that Digital Engineering could provide.

This research contributes empirical evidence to a domain currently characterized by theoretical proposition rather than data. The perception data collected from working professionals enables assessment of whether the theoretical framework connecting Digital Engineering capabilities to documented IT and information assurance challenges resonates with practitioners who experience those challenges directly. Findings indicating that professionals recognize value in Digital Engineering capabilities would validate the theoretical premise and support subsequent implementation research. Findings indicating limited awareness or skepticism would redirect the research agenda toward education initiatives or reconsideration of transferability assumptions. Either outcome advances the academic understanding of Digital Engineering's potential applicability beyond its established defense and aerospace foundations.

\subsection{Industry Significance}

For industry practitioners, this research provides insight into how their peers perceive Digital Engineering capabilities. Organizations considering Digital Engineering adoption can benefit from understanding current awareness levels and perceived value within the professional community. If research reveals widespread recognition of Digital Engineering value, organizations may find receptive audiences for adoption initiatives. If research reveals limited awareness or skepticism, organizations can anticipate the education and change management challenges that adoption would require.

The research also identifies which specific Digital Engineering capabilities professionals perceive as most valuable for their work. This information enables tool vendors, service providers, and standards organizations to focus development and communication efforts on the capabilities that practitioners recognize as addressing their needs. Understanding professional perceptions enables more effective resource allocation across the ecosystem supporting Digital Engineering adoption.

\subsection{Commonwealth Significance}

The commonwealth significance of this research relates to national security and protection of societal infrastructure. Federal information systems and national security systems protect information assets and enable government operations upon which citizens depend. Organizations operating these systems face the challenges documented throughout this dissertation: maintaining accurate documentation, implementing consistent security controls, and verifying compliance across complex technical environments.

Digital threads, as demonstrated in defense applications, enhance compliance verification and security assurance by providing authoritative traceability---verified connections between security requirements, control implementations, and compliance evidence~\cite{DoD_DE_Strategy_2018}. Operators of federal and national security systems must demonstrate compliance with numerous regulatory frameworks and security standards, often requiring extensive manual effort to collect evidence and prepare for audits. Digital Engineering practices offer the potential to reduce the burden of compliance verification while improving the accuracy and currency of compliance documentation, enabling organizations to redirect resources toward proactive security improvements rather than compliance documentation.

The ability to understand first, second, and third order impacts of security incidents carries particular significance for critical infrastructure protection. When adversaries compromise systems supporting government functions or critical infrastructure, defenders must rapidly assess the scope of compromise and potential cascading effects. Digital Engineering practices are designed to provide the visibility and traceability that enable rapid, accurate impact assessment---capabilities that current approaches demonstrably fail to provide. Whether these capabilities can be realized effectively outside defense and aerospace contexts remains an open question that this research begins to address through professional perception data.

\subsection{Societal Significance}

Beyond organizations operating national security systems, Digital Engineering capabilities may benefit organizations serving communities with limited resources. Healthcare providers, educational institutions, social service organizations, and other entities serving underserved populations face the same information assurance and IT service delivery challenges as large enterprises, often with fewer resources to address them.

If Digital Engineering practices prove transferable beyond defense and aerospace contexts, their documentation and traceability capabilities could reduce the time and specialized knowledge required for compliance verification, potentially making it more feasible for smaller organizations to demonstrate regulatory compliance and security effectiveness to funding agencies, oversight bodies, and stakeholders. Many organizations serving underserved populations must comply with privacy regulations, security standards, and funding requirements that demand documented evidence of security controls and compliance measures. Digital Engineering practices may reduce the burden of generating and maintaining compliance documentation, enabling organizations to redirect limited staff time and resources toward direct service provision rather than compliance administration.

The potential for Digital Engineering to extend sophisticated security and documentation capabilities to resource-constrained organizations represents a motivating consideration for this research. Currently, enterprise architecture, authoritative traceability, and model-based documentation remain accessible primarily to large organizations with specialized expertise and dedicated budgets. This research does not directly investigate adoption within resource-constrained organizations. However, by establishing whether IT and information assurance professionals broadly perceive value in Digital Engineering capabilities, the findings create a foundation for subsequent research examining practical applicability across diverse organizational contexts, including those serving underrepresented populations.

\section{Chapter Summary}

This chapter has established the context for investigating professional awareness and perceptions of Digital Engineering capabilities within information assurance and IT service management domains. The discussion identified the challenges organizations face in maintaining accurate system documentation, implementing consistent security controls, and delivering reliable IT services using traditional document-centric approaches. Digital Engineering, with its four pillars of Model-Based Systems Engineering, digital threads, digital twin technology, and Product Lifecycle Management, offers capabilities that have demonstrated value in defense and aerospace contexts and that may address analogous gaps in enterprise IT and information assurance practice. The institutional endorsement of Digital Engineering by the Department of Defense, NASA, INCOSE, and standards bodies including the Object Management Group and ISO establishes the maturity and authoritative standing of these methodologies, while enterprise architecture frameworks---particularly the Unified Architecture Framework---provide the structural foundation for implementation.

The research questions focus upon measuring professional awareness of Digital Engineering capabilities, assessing whether professionals perceive these capabilities as valuable for their work, and determining whether professionals believe Digital Engineering practices could enhance their effectiveness in meeting compliance requirements and delivering IT services. The significance of this research spans academic contribution through addressing identified literature gaps, industry benefit through informing adoption strategies, commonwealth value through enhancing protection of government systems, and societal benefit through potentially enabling better security capabilities for organizations serving underserved populations.

The research targets IT and information assurance professionals across diverse organizational contexts, enabling assessment of awareness and perceived value across the professional community. While the defense and aerospace sectors have demonstrated Digital Engineering benefits, this research investigates whether professionals in other sectors recognize potential value in these capabilities for their work. The findings will inform whether Digital Engineering methodologies developed for defense applications might benefit organizations across all sectors, including those serving underrepresented populations with limited resources for compliance documentation and security administration.

Chapter 2 presents the literature review examining existing research across Digital Engineering, information assurance, and IT service management domains. The review establishes the theoretical framework for this research while documenting the research gaps that this investigation begins to address through a sustained analytical synthesis that identifies not merely what the literature contains but what debates remain unresolved, what tensions persist between proven capability and limited adoption, and why the present research is necessary.