\chapter{\leavevmode\newline Literature Review}\label{chap:chapter_2}

The literature on Digital Engineering tells a story of proven capability confined within disciplinary walls. Model-Based Systems Engineering, digital threads, digital twins, and Product Lifecycle Management have matured through decades of application in defense and aerospace, accumulating institutional endorsement from the Department of War, NASA, and INCOSE alongside rare but compelling empirical evidence of return on investment. Yet this maturation has occurred almost entirely within the systems engineering discipline. Enterprise IT infrastructure management and Information Assurance---domains that confront structurally analogous challenges of documentation accuracy, configuration visibility, compliance traceability, and change coordination---have developed independently, producing their own frameworks, their own tools, and their own patterns of persistent failure. The result is a pronounced research gap: apart from a preliminary reference model by \citeauthor{Bonar_Hastings_2024}~\cite{Bonar_Hastings_2024} and a conceptual framework for DevSecOps security assurance from the Carnegie Mellon Software Engineering Institute~\cite{CMU_SEI_DevSecOps_2023}, no peer-reviewed research addresses the application of Digital Engineering methodologies to enterprise IT infrastructure, IT Service Management, or Information Assurance programs broadly.

Why that gap persists and why it matters forms the central inquiry of this chapter. Proceeding through an analytical synthesis of research spanning MBSE value evidence, adoption dynamics, cross-disciplinary transfer barriers, digital twin security applications, compliance automation, IT service management failures, and enterprise visibility challenges. Rather than cataloging what exists, the review foregrounds the debates, tensions, and unresolved questions that position this dissertation's investigation within the academic conversation. Three interrelated arguments emerge. First, the evidence base for MBSE value, while growing, remains disproportionately reliant upon perceived rather than measured outcomes, creating economic uncertainty that discourages adoption beyond established domains. Second, adoption research demonstrates that perceptions---not objective technical merit---drive adoption decisions, yet no empirical data exist on how IT and Information Assurance professionals perceive Digital Engineering capabilities. Third, compliance frameworks and IT service management standards explicitly require capabilities that Digital Engineering provides, yet the academic community has not investigated whether Digital Engineering could fulfill these requirements more effectively than traditional approaches that demonstrably fail.

\section{The Evidence Paradox: Demonstrated Value Without Sufficient Proof}

The case for Model-Based Systems Engineering rests upon a paradox that shapes the entire adoption landscape: practitioners and organizations widely perceive MBSE as valuable, yet rigorous empirical measurement of that value remains exceptionally rare. Understanding this paradox is essential because it explains both why MBSE has succeeded within systems engineering and why it has not expanded beyond it. Organizations already embedded in the systems engineering discipline can rely upon institutional experience and professional networks to validate MBSE investment. Organizations outside the discipline---including enterprise IT and Information Assurance---lack comparable experiential evidence and must rely upon published research that, as the literature reveals, provides insufficient quantitative justification.

Among the most comprehensive assessments of MBSE evidence, \citeauthor{Henderson_Salado_2021} conducted a systematic literature review categorizing reported benefits into four evidence types: measured, observed, perceived, and referenced~\cite{Henderson_Salado_2021}. Their finding that approximately two-thirds of claimed MBSE benefits lack empirical measurement and instead rely upon perceived or referenced evidence carries profound implications for adoption decisions. Organizations evaluating MBSE investment discover that the research literature offers abundant testimony to MBSE's promise but limited quantified outcomes upon which to build business cases. \citeauthor{Madni_Sievers_2018} reached a similar conclusion in their review of MBSE motivation and research opportunities, arguing that the value proposition requires demonstration through real-world applications and that further advancements remain necessary before broader adoption can occur~\cite{Madni_Sievers_2018}. More recently, \citeauthor{Wooley_Womack_2025} analyzed adoption, benefits, and challenges across the Digital Engineering research corpus and explicitly noted the absence of research addressing enterprise IT infrastructure or Information Assurance applications~\cite{Wooley_Womack_2025}. Their finding validates that the research gap identified in this dissertation reflects the actual state of academic literature rather than incomplete literature search.

Among the rare exceptions to the measurement deficit, the work of \citeauthor{Rogers_Mitchell_2021} stands as the most frequently cited quantified evidence of MBSE return on investment~\cite{Rogers_Mitchell_2021}. Their case study examined the Submarine Warfare Federated Tactical Systems program---a rapidly evolving combat system of systems comprising over ten million source lines of code deployed across 104 submarines---following a three million dollar MBSE investment over two and a half years. An apples-to-apples comparison between legacy document-centric processes and the MBSE approach revealed that systems engineering hours per requirement decreased from 12.1 to 9.9, representing an eighteen percent improvement in efficiency that exceeded the thirteen percent improvement projected by an earlier pilot study. The MBSE approach handled forty-two percent more interface requirements changes while consuming only sixteen percent more hours, reduced total interface defects by nine percent, and shifted eighteen percent of defects to earlier discovery phases where correction costs between 1.6 and 4 times less than defects discovered during platform integration testing. Supplementary data demonstrated thirty percent more baselines produced monthly and sixty percent more integrated subsystems and combat system variants managed within constant resources.

Significant not merely for their magnitude but for their scarcity, these results merit careful examination. \citeauthor{Henderson_Salado_2021} identified the Rogers and Mitchell study as one of only two papers in the literature reporting measured MBSE return on investment evidence~\cite{Henderson_Salado_2021}. As a practical consequence, organizations contemplating MBSE adoption must extrapolate from a remarkably thin evidence base. For enterprise IT organizations operating on annual budgets with continuous delivery expectations, this evidentiary deficit creates a rational basis for hesitation: the demonstrated benefits derive from a naval combat system program operating under a level-of-effort contract structure fundamentally different from enterprise IT budget models. Whether comparable returns would materialize in enterprise IT contexts remains entirely unexamined.

Earlier systematic reviews reinforce this pattern. \citeauthor{Wolny_2020} reviewed thirteen years of SysML research, mapping the landscape of model-based methods and their empirical evidence~\cite{Wolny_2020}. \citeauthor{Chami_Bruel_2018} surveyed MBSE tools and applications, finding consistent reports of improved requirements traceability and stakeholder communication alongside persistent concerns about tool complexity and organizational adoption challenges~\cite{Chami_Bruel_2018}. Research by \citeauthor{gregory2019model} examined model-based engineering practices within defense programs, documenting improved requirements traceability and more effective design reviews but also identifying organizational and technical barriers that limit realization of theoretical benefits~\cite{gregory2019model}. Collectively, these reviews establish that MBSE has demonstrated value across multiple domains while simultaneously documenting two critical limitations: the evidence remains predominantly qualitative, and the domains of application remain overwhelmingly confined to aerospace and defense.

The tension between demonstrated capability and insufficient proof creates what might be termed an adoption credibility gap. Within the systems engineering discipline, professional experience and institutional knowledge compensate for the measurement deficit. Systems engineers who have used MBSE tools can observe improvements in their own work, creating the experiential validation that published evidence lacks. Outside the discipline, this compensatory mechanism does not operate. IT and Information Assurance professionals have no experiential basis for evaluating MBSE claims. The literature provides them with compelling arguments but insufficient evidence---a combination that technology adoption research suggests is inadequate for driving adoption decisions.

\section{Adoption as a Perception Problem}

If the evidence paradox explains why MBSE has not expanded through rational economic calculation, the adoption literature reveals a complementary explanation rooted in perception dynamics. A growing body of empirical research demonstrates that MBSE adoption decisions are driven more by how professionals perceive the technology than by its objective characteristics. This finding carries direct implications for the present research: if perceptions govern adoption even among systems engineering professionals who work with models daily, understanding perceptions among IT and Information Assurance professionals---who have never systematically encountered these methodologies---becomes a prerequisite for any meaningful discussion of cross-disciplinary transfer.

The theoretical foundation for this argument derives from \citeauthor{Call_Herber_2022}, who mapped Everett Rogers' Diffusion of Innovations theory to MBSE adoption dynamics~\cite{Call_Herber_2022}. Rogers' framework identifies five perceived attributes of innovations---relative advantage, compatibility, complexity, trialability, and observability---that collectively account for forty-nine to eighty-seven percent of variance in adoption rates across innovation research. The critical theoretical insight, and the one most consequential for the present research, is that \textit{perceptions} of these attributes, not the attributes themselves, drive adoption behavior. \citeauthor{Call_Herber_2022} invoke W.~I.~Thomas's dictum that ``if [people] perceive situations as real, they are real in their consequences,'' arguing that shaping how MBSE attributes are perceived can accelerate adoption rates more effectively than improving the attributes themselves~\cite{Call_Herber_2022}.

\citeauthor{Call_2024} tested this theoretical mapping empirically through a survey of approximately 270 systems engineering professionals distributed through INCOSE and professional networks~\cite{Call_2024}. Across six subpopulation comparisons---involved versus not involved in MBSE efforts, model users versus non-users, and respondents subject to Digital Engineering mandates versus those who are not---the study examined perceptions. Respondents broadly recognized MBSE's relative advantage in improving data quality and traceability. However, perceived compatibility with existing practices and perceived complexity emerged as significant barriers that suppressed adoption even when practitioners acknowledged objective value.

The most striking finding involves trialability. Respondents not involved in MBSE efforts reported dramatically lower access to trial opportunities and tools, with a chi-squared test result of $p=0.00$ for the involved/non-involved comparison~\cite{Call_2024}. What \citeauthor{Call_2024} characterize as a barrier-reinforcing cycle emerges: professionals who have not experienced MBSE cannot access the trial opportunities that would facilitate informed adoption decisions, while those who have experienced MBSE report substantially more favorable perceptions. Further complicating adoption, the study characterizes MBSE as a ``preventative innovation'' whose advantages derive from preventing problems---inconsistencies, documentation errors, rework---rather than producing visible new benefits. Depressed perceived relative advantage and observability compound adoption barriers even when objective value exists.

This perception-adoption dynamic has profound implications for cross-disciplinary transfer. If systems engineering professionals---practitioners who work within the discipline that developed MBSE---exhibit perception gaps that impede adoption, the perception barriers among IT and Information Assurance professionals are likely to be substantially greater. These professionals operate outside the systems engineering discipline entirely. They have no professional exposure to MBSE concepts, no institutional networks sharing MBSE experiences, and no trial opportunities through which favorable perceptions might develop. The present research addresses this gap by measuring perceptions in a population where no empirical data currently exist.

Complementary evidence from \citeauthor{Henderson_McDermott_Salado_2024} deepens understanding of adoption dynamics through a mixed-methods study combining systematic literature review with eighteen semi-structured practitioner interviews from organizations attempting MBSE adoption~\cite{Henderson_McDermott_Salado_2024}. From the literature review, over six hundred individual lessons learned were extracted from forty-six published papers, coded into categories spanning communication, model definition, organizational strategy, and technical implementation. Interview findings illuminate the human dimensions of adoption that quantitative studies cannot fully capture. Perhaps most directly relevant to the question of professional awareness, approximately twenty-two percent of interview participants---all systems engineering professionals---could not convey a clear definition of MBSE\@. One participant captured the prevailing confusion: ``Most people think of MBSE as being synonymous with a specific tool'' and ``don't understand how MBSE fits into DE or what it really does other than it is modeling instead of documents''~\cite{Henderson_McDermott_Salado_2024}. Another stated plainly that ``if they aren't familiar with MBSE, they're not going to use it.''

The organizational lessons proved equally revealing. For both organizations in the study whose adoption efforts failed, the driving barrier was lack of management support. Middle management resistance proved particularly insidious because executives endorsed MBSE while middle managers could disregard the endorsement without operational consequences. Successful adopters commonly established core MBSE teams---communities of practice or centers of excellence---and implemented role-based training at four levels: model reviewers for leaders and decision-makers, developers and modelers, architects for senior engineers, and administrators for IT and software support. A reinforcing dynamic emerged: greater stakeholder exposure to models produced more perceived benefits, which generated more organizational buy-in, which enabled further exposure. This virtuous cycle operates within organizations that have initiated adoption but cannot initiate itself in domains where MBSE has never been introduced.

\citeauthor{Henderson_Salado_2024_Org} extended these findings by examining how organizational structure variables correlate with MBSE adoption outcomes through a survey of fifty-one practitioners~\cite{Henderson_Salado_2024_Org}. The results reveal a pattern with direct implications for enterprise IT contexts. Flexibility and interconnectedness showed the strongest and most consistent positive correlations with both adoption process and implementation variables. Formalization---documented procedures and processes---correlated positively, a somewhat counterintuitive finding suggesting that governance frameworks support rather than hinder adoption when combined with flexibility. Centralization correlated negatively with adoption and implementation, as did large organizational size and high vertical differentiation. These structural findings suggest that enterprise IT organizations characterized by rigid hierarchies, centralized governance, and siloed operations may face structural impediments to Digital Engineering adoption that compound the perception barriers documented by \citeauthor{Call_2024}.

Research beyond the systems engineering discipline corroborates these dynamics. \citeauthor{Vogelsang_2017} conducted a qualitative study of twenty interviews across ten companies in the embedded systems industry---outside the defense and aerospace sector where MBSE originated~\cite{Vogelsang_2017}. Their findings identified immature tooling, return on investment uncertainty, and migration fears as key barriers, confirming that the adoption challenges documented within systems engineering intensify when the technology crosses disciplinary boundaries. \citeauthor{Campagna_2024} examined strategic adoption of digital innovations more broadly, finding that digital transformation requires coordinated enterprise-level application rather than bottom-up adoption of individual technologies~\cite{Campagna_2024}. The research identified twelve strategic adoption influencers and noted that adoption research focuses upon individual technologies rather than integrated digital transformation---a finding that explains why platform-level Digital Engineering adoption within defense programs has not expanded to enterprise IT functions operating separately from program organizations.

The adoption literature thus establishes a critical precondition for the present research. Technology adoption is governed by perceptions. Perceptions among systems engineering professionals---the population most favorably positioned to appreciate MBSE---remain mixed and marked by significant awareness deficits. No comparable perception data exist for IT and Information Assurance professionals. Until such data are collected, any discussion of Digital Engineering adoption beyond defense and aerospace rests upon assumption rather than evidence. This dissertation addresses that evidentiary gap.

\section{Crossing Disciplinary Boundaries: Digital Engineering Beyond Its Origins}

Concentrated within aerospace and defense, Digital Engineering raises an unresolved question that the literature has acknowledged but not adequately investigated: can these methodologies transfer effectively to domains with different professional cultures, tool ecosystems, and economic structures? Preliminary evidence is suggestive but fragmented, consisting of isolated applications to security domains, a conceptual framework from an authoritative institution, and a single systematic mapping study of model-based security engineering. No sustained research program has examined cross-disciplinary transfer to enterprise IT or Information Assurance.

\citeauthor{Bone_Blackburn_2019} described the Systems Engineering Research Center's research effort underlying the DoD Digital Engineering initiative, identifying three key enablers for transformation: IT infrastructure, workforce development, and policy~\cite{Bone_Blackburn_2019}. Though defense-originated, their analysis addressed cross-cutting adoption dynamics and workforce transformation challenges applicable to any domain adopting Digital Engineering. The transformation they described requires coordinated investment across technology, people, and governance---precisely the integrated approach that enterprise IT organizations, structured around operational silos and annual budget cycles, struggle to sustain.

Perhaps the most substantive bridge between Digital Engineering and Information Assurance comes from the Carnegie Mellon Software Engineering Institute. \citeauthor{CMU_SEI_DevSecOps_2023} argued that DevSecOps pipelines constitute complex socio-technical systems---comprising independently developed and maintained, physically and logically distributed, interoperable components---that require systems engineering treatment~\cite{CMU_SEI_DevSecOps_2023}. Tight integration of business mission, capability delivery, and products ``increases the attack surface of the product under development,'' and as organizations adopt DevSecOps tools and techniques with increased coupling between products and the tools used to build them, ``the attack surface continues to grow, incorporating segments of the development environment itself.'' The research developed a DevSecOps Platform Independent Model using the Unified Architecture Framework and SysML to model pipeline security properties, demonstrating how to construct cybersecurity assurance cases from model-based representations.

The SEI framework proposed a conceptual shift with direct relevance to this dissertation: from process-based to property-based security assurance. Traditional cybersecurity assurance relies upon process-based standards such as the NIST Risk Management Framework and SP~800-53 security controls. As systems become more complicated and interconnected, \citeauthor{CMU_SEI_DevSecOps_2023} argued, ``process-based standards fail to assure system owners that the system functions only as intended under all operational circumstances''~\cite{CMU_SEI_DevSecOps_2023}. When MBSE-based assurance is implemented effectively, ``the overall risks associated with the DevSecOps pipeline and associated products will be reduced, and the compliance and legal requirements will naturally be addressed within the engineering lifecycle.'' This finding directly supports the theoretical premise of this dissertation: that Digital Engineering capabilities can integrate security engineering into development and operational processes rather than treating compliance as a separate activity conducted after the fact.

However, the SEI framework represents a conceptual and architectural argument rather than empirical validation. Technical feasibility is demonstrated through detailed modeling of configuration management capabilities, but quantitative effectiveness data are absent. Designed for heavily regulated and cybersecurity-constrained environments including defense, banking, and healthcare---domains where Information Assurance professionals operate and where compliance verification demands consume substantial organizational resources---the framework leaves the critical question unanswered: does MBSE-based security assurance deliver measurable improvements over traditional compliance approaches in practice?

Other researchers have probed adjacent territory without directly addressing enterprise IT\@. \citeauthor{Huff_Medal_2019} developed an MBSE methodology for critical infrastructure vulnerability assessment and decision analysis, using DoDAF-based modeling to link regulatory requirements, system architecture, and attack vectors~\cite{Huff_Medal_2019}. Their work demonstrates MBSE application to infrastructure protection and security assessment beyond traditional platform engineering but does not extend to enterprise IT infrastructure management. \citeauthor{Mazeika_Butleris_2020} presented a UML-based MBSE security profile conforming to ISO/IEC 27001 and found through a feasibility survey of ten engineering companies that security aspects are inadequately addressed by standard SysML and popular MBSE methods~\cite{Mazeika_Butleris_2020}. A tool gap identified by this finding would impede adoption even if organizational and perception barriers were overcome: the modeling languages themselves require extension to accommodate security constructs that enterprise IT and Information Assurance demand. \citeauthor{Apvrille_Roudier_2015} proposed SysML-SecA, combining SysML with security analysis techniques for integrated threat modeling~\cite{Apvrille_Roudier_2015}. Collectively, these preliminary investigations establish that researchers have recognized the potential of MBSE for security applications while simultaneously documenting the technical, methodological, and empirical gaps that separate potential from realization.

\citeauthor{Nguyen_Ali_Yue_2017} conducted a systematic mapping of forty-eight primary studies on model-based security engineering for cyber-physical systems~\cite{Nguyen_Ali_Yue_2017}. The study found that most research uses domain-specific languages or UML, focuses upon early lifecycle stages, and addresses threats, attacks, and vulnerabilities generically. This mapping reveals a field in its formative stages---one where foundational concepts are being established but where mature, validated methodologies for operational security engineering have not yet emerged. The gap between model-based security engineering research and operational Information Assurance practice remains substantial.

The academic literature on MBSE applications thus presents a landscape of expanding interest constrained by limited evidence. Researchers have begun exploring applications to security domains, infrastructure protection, and compliance frameworks. Authoritative institutions have produced conceptual frameworks demonstrating technical feasibility. Yet the enterprise IT context---where organizations manage heterogeneous infrastructure, maintain compliance across multiple regulatory frameworks, and coordinate service delivery across organizational silos---remains conspicuously absent from the research. The frameworks exist; the methodologies have matured; the tools have proliferated. But the research community has not applied these capabilities to the domains where visibility and documentation challenges persist most acutely.

\section{Digital Twins as Emerging Security Tools}

Digital twin technology has generated substantial research interest in cybersecurity applications, creating a parallel track to MBSE-based security engineering that the literature has not yet integrated into a coherent Digital Engineering narrative. The growing body of research on digital twins for security purposes represents both an opportunity and an analytical challenge: it demonstrates that Digital Engineering concepts are penetrating security domains, but it does so through a technology-specific lens that misses the integrated approach---combining MBSE, digital threads, digital twins, and PLM---that characterizes Digital Engineering as a discipline.

\citeauthor{Grieves_2023} traces the evolution of digital twin concepts from Product Lifecycle Management origins through contemporary applications, positioning digital twins as the integration of physical and virtual systems that enables analysis, optimization, and prediction~\cite{Grieves_2023}. The foundational distinction between digital twins and traditional simulation lies in synchronization: digital twins maintain continuous data exchange with their physical or logical counterparts, enabling operational decision-making based upon current rather than documented system states. \citeauthor{madni2018leveraging} provided an influential framework for leveraging digital twins in systems engineering contexts, establishing the conceptual architecture that subsequent security applications adapted~\cite{madni2018leveraging}.

\citeauthor{ElHajj_2024} conducted a systematic literature review analyzing sixty-seven papers published between 2018 and 2023 examining how digital twins enhance security in Industry 4.0 applications~\cite{ElHajj_2024}. Covering intrusion detection, vulnerability assessment, cyber range simulation, and threat intelligence applications, the review identified enabling technologies---machine learning, blockchain, and 5G---used in conjunction with digital twins for security purposes. \citeauthor{Alhumam_2025} extended this analysis in a comprehensive review categorizing digital twin cybersecurity studies by technique type and digital twin level---component, process, asset, system, and network-of-systems---assessing risk levels from medium to very high depending upon twin type and industry sector~\cite{Alhumam_2025}. Collectively, these systematic reviews confirm that digital twin security applications have achieved sufficient research volume to warrant meta-analysis, indicating a maturing research area.

Within this expanding literature, several streams bear directly upon Information Assurance and enterprise IT applications. \citeauthor{Eckhart_Ekelhart_2019} reviewed digital twins for cyber-physical systems security, examining how virtual replicas can support security testing and analysis without affecting operational systems~\cite{Eckhart_Ekelhart_2019}. \citeauthor{Vielberth_2021} proposed a digital twin-based cyber range for Security Operations Center analyst training, demonstrating practical application of digital twin concepts to security workforce development~\cite{Vielberth_2021}. \citeauthor{Dietz_Pernul_2020} examined digital twins for enterprise security, exploring how organizations might employ virtual representations to understand and manage security postures~\cite{Dietz_Pernul_2020}. \citeauthor{Karaarslan_Babiker_2021} examined digital twin security threats and countermeasures, addressing the bidirectional security challenge: digital twins can enhance security while simultaneously introducing new attack surfaces~\cite{Karaarslan_Babiker_2021}.

Standards development further indicates maturing technology readiness. \citeauthor{Shao_2021} examined ISO 23247 and IEC 62832 standards for digital twin frameworks~\cite{Shao_2021}. \citeauthor{Shao_ISO_23247_2023} provided additional analysis of ISO 23247's four-part structure~\cite{Shao_ISO_23247_2023}. The Internet Engineering Task Force has published a draft reference architecture for network infrastructure digital twins~\cite{IETF_NDT_2024}. Together, these standardization efforts establish interoperability requirements that reduce vendor lock-in concerns and enable integration across implementations.

NIST's engagement with digital twin technology illuminates the current state of institutional thinking. NIST Internal Report 8356 addresses cybersecurity challenges and trust considerations for digital twin implementations~\cite{NIST_IR_8356_2025}. Significantly, this publication addresses security considerations \textit{for} systems employing digital twins rather than digital twin applications \textit{to} Information Assurance. A revealing distinction emerges: NIST examines how to secure digital twin implementations rather than how digital twins might enhance security posture visibility or compliance verification. Current institutional research thus frames digital twins as systems to be secured rather than as tools for improving security operations.

Academic research on open source digital twin frameworks adds another dimension to the adoption question. \citeauthor{Gil_2024} conducted a systematic survey of open source digital twin frameworks, analyzing fourteen frameworks against criteria derived from ISO 23247 standards and finding significant variation in maturity, documentation quality, and community support~\cite{Gil_2024}. \citeauthor{Autiosalo_Siegel_Tammi_2021} introduced Twinbase, an open source server for the Digital Twin Web concept~\cite{Autiosalo_Siegel_Tammi_2021}. While these open source options reduce economic barriers to adoption, they do not resolve the more fundamental challenge: digital twin applications for enterprise IT contexts lack the academic research and validated methodologies that would guide adoption decisions.

However, framing digital twins exclusively as defensive or simulation tools obscures a critical dimension of the technology: the bidirectional data streams that enable digital twin functionality simultaneously constitute primary attack surfaces. \citeauthor{Alcaraz_Lopez_2022} conducted the most comprehensive survey of digital twin security threats to date, classifying attack vectors across both digital and physical layers and cataloging threats including software exploitation, privilege escalation, denial of service, data extraction, and man-in-the-middle attacks targeting the communication channel between physical systems and their virtual counterparts~\cite{Alcaraz_Lopez_2022}. Their analysis demonstrates that adversaries who compromise a digital twin can extract private information---including services, dynamics data, configurations, states, and security credentials---that may subsequently be weaponized for cyber espionage or leveraged to identify and exploit vulnerabilities in production systems. Attacks cascade bidirectionally: compromise of the digital representation can propagate to physical systems, while manipulation of physical components can corrupt digital twin fidelity.

\citeauthor{Suhail_Iqbal_Jurdak_2024} confronted the assumption that digital twins serve purely security-enhancing functions, introducing the concept of malicious digital twins that adversaries can exploit as observation platforms to covertly learn physical system behavior through model analysis~\cite{Suhail_Iqbal_Jurdak_2024}. Their analysis identifies a fundamental tension: high-fidelity digital twins required for accurate security analysis simultaneously create high-fidelity attack surfaces from which adversaries can extract operational intelligence. Malicious actors can interrupt digital threads connecting physical counterparts, cause information leakage exposing system functioning, and manipulate physical operations through compromised twin interfaces. In a related investigation, \citeauthor{Suhail_ENIGMA_2023} documented a two-stage adversary strategy in which attackers first place the digital twin into a malicious state as a data acquisition source, then exploit that compromised state to covertly manipulate the underlying physical system~\cite{Suhail_ENIGMA_2023}. Cyclic state updates between the twin and the physical process amplify the potential for cascading damage.

At the infrastructure level, the communication layer connecting physical and digital twins represents the most concentrated attack surface. \citeauthor{Rodrigues_2025} identified the data connection layer as the central nervous system of digital twin architecture in Industry 4.0 environments and demonstrated that denial-of-service, man-in-the-middle, and intrusion attacks targeting bidirectional data streams via OPC~UA and MQTT channels can corrupt twin fidelity and trigger hazardous actions in physical systems~\cite{Rodrigues_2025}. Their proposed data rate monitoring approach achieved complete detection across all three attack categories, but the underlying finding carries broader significance: the IT/OT convergence that digital twins facilitate introduces security challenges distinct from traditional IT environments, including resource-constrained devices, heterogeneous protocols, and legacy system integration requirements. \citeauthor{Gehrmann_Gunnarsson_2020} reached a complementary conclusion, demonstrating that opening industrial automation and control system functions through digital twin interfaces creates threat vectors for which traditional ICS security approaches prove insufficient~\cite{Gehrmann_Gunnarsson_2020}. \citeauthor{Eckhart_Brenner_2023} systematized security-enhancing digital twins while acknowledging that twins accurately reflecting physical devices---including their vulnerabilities---simultaneously create information assets that adversaries could exploit, and that digital twins deployed as honeypots face the paradox of providing adversaries with detailed knowledge of real system behavior~\cite{Eckhart_Brenner_2023}.

These findings carry direct implications for the present research. Organizations contemplating digital twin adoption for Information Assurance purposes must address a dual challenge: securing the digital twin implementation itself while leveraging its capabilities for security improvement. Digital twins that provide visibility into system configurations, dependencies, and security postures also concentrate precisely the information that adversaries seek. Appropriate protections must ensure that threat actors cannot compromise the digital twin or extract knowledge from it that could be used to exploit production environments. NIST's framing of digital twins primarily as systems requiring security---rather than as security tools---reflects institutional recognition of this duality, even if the research community has not yet developed comprehensive frameworks for managing the tension between digital twin utility and digital twin vulnerability in enterprise contexts.

The digital twin security literature thus reveals a field developing in parallel with but largely disconnected from the broader Digital Engineering discourse. Researchers explore digital twins as security tools without embedding their work within the Digital Engineering framework that integrates MBSE, digital threads, and PLM into a coherent discipline. This disconnection means that individual digital twin security capabilities are investigated in isolation rather than as components of the integrated approach that defense and aerospace organizations employ. Whether integrating digital twin security capabilities within a comprehensive Digital Engineering framework would produce greater value than isolated implementations remains an open question---one that the absence of enterprise IT research leaves entirely unaddressed.

\section{The Compliance Imperative and Its Unfulfilled Requirements}\label{sec:compliance_imperative}

Compliance frameworks create requirements that Digital Engineering could address, yet no research examines Digital Engineering approaches to satisfying these requirements. This disconnect between what frameworks demand and what current practices deliver represents one of the most compelling arguments for investigating Digital Engineering application to Information Assurance.

Documented in Special Publication 800-37 Revision 2, the NIST Risk Management Framework provides the authoritative approach to managing security and privacy risk for federal information systems through seven iterative steps: prepare, categorize, select, implement, assess, authorize, and monitor~\cite{NIST_SP_800_37_R2_2018}. Critically, the RMF explicitly requires enterprise architecture integration during the prepare step. Yet compliance with this requirement assumes capabilities that organizations demonstrably lack: the ability to maintain accurate, current documentation of enterprise architecture that reflects operational reality. NIST Special Publication 800-53 Revision 5 compounds these demands through specific controls requiring enterprise architecture capabilities: PL-2 requires security plans consistent with enterprise architecture; PL-8 requires security architecture development; PM-7 establishes enterprise architecture requirements; CM-2 requires documented baselines; CM-8 requires accurate system component inventory; SA-17 requires design specifications consistent with enterprise architecture~\cite{Force_2020}. Each control establishes compliance obligations that Digital Engineering could address. Yet no research examines Digital Engineering approaches to satisfying these specific controls.

The Committee on National Security Systems Instruction 1253 extends these requirements to national security systems, where documentation and visibility challenges compound under additional constraints~\cite{CNSSI_1253_2022}. Organizations operating outside federal requirements may employ ISO/IEC 27001:2022 for information security management~\cite{ISO27001-2023}. \citeauthor{Mazeika_Butleris_2020} found that standard MBSE methods inadequately address ISO 27001 security requirements, identifying a specific gap between what compliance frameworks demand and what current modeling approaches provide~\cite{Mazeika_Butleris_2020}. These alternative frameworks share a common characteristic with NIST guidance: they assume documentation accuracy and visibility capabilities that organizations struggle to maintain.

Beyond federal information systems and national security systems, an expanding compliance landscape governs nonfederal organizations that process, store, or transmit Controlled Unclassified Information (CUI) on behalf of government agencies. NIST Special Publication 800-171 Revision 3 establishes security requirements for protecting CUI in nonfederal systems and organizations~\cite{NIST_SP_800_171_R3_2024}. Revision 3, finalized in May 2024, restructured the framework from fourteen to seventeen security requirement families---adding Planning, System and Services Acquisition, and Supply Chain Risk Management---while reducing individual requirements from 110 to 97 but increasing the specificity of each requirement to 266 individual control items. Significantly, Revision 3 aligned its control structure directly with SP~800-53 Revision 5 as the single authoritative source and introduced forty-nine Organization-Defined Parameters that provide implementation flexibility while enabling automated compliance assessment through machine-readable formats such as OSCAL\@. SP~800-171's architecture is substantially more amenable to automation than its predecessor, and the closer alignment with SP~800-53 enables organizations to leverage common tooling across federal and nonfederal compliance obligations.

NIST Special Publication 800-172 supplements the SP~800-171 baseline with thirty-five enhanced security requirements designed to protect CUI associated with critical programs and high-value assets against Advanced Persistent Threats~\cite{NIST_SP_800_172_2021}. SP~800-172 operates upon a three-pillar defense strategy: penetration-resistant architecture, damage-limiting operations, and cyber resiliency and survivability. Enhanced requirements span access control, configuration management, identification and authentication, incident response, risk assessment, situational awareness, and system protection---each demanding capabilities that manual processes and static documentation struggle to sustain at the required tempo. Federal agencies select applicable requirements based upon mission needs and risk assessment, creating a tailored compliance burden that automated, model-based approaches could reduce. A Revision 3 update currently in development expands scope from confidentiality protection alone to encompass confidentiality, integrity, and availability, further increasing the documentation and verification demands upon implementing organizations.

The Cybersecurity Maturity Model Certification (CMMC) 2.0 framework operationalizes these NIST requirements within the Department of War's defense industrial base through a three-tiered certification structure~\cite{DoD_CMMC_2024}. Level 1 requires fifteen foundational practices for Federal Contract Information protection through annual self-assessment. Level 2 mandates compliance with the 110 requirements of SP~800-171 Revision 2, assessed through triennial third-party evaluation by CMMC Third-Party Assessment Organizations (C3PAOs). Level 3 adds twenty-four enhanced requirements drawn from SP~800-172, assessed by the Defense Industrial Base Cybersecurity Assessment Center. Phase 1 implementation commenced in November 2025, with full implementation across all contracts involving Federal Contract Information or Controlled Unclassified Information expected by November 2028. CMMC represents a fundamental shift from self-attestation to verified compliance, imposing documentation and evidence requirements that intensify the challenges organizations already face in maintaining accurate, current security documentation.

The intersection of these compliance frameworks with emerging technologies---particularly artificial intelligence, DevSecOps automation, and continuous monitoring---has received limited but growing academic attention. \citeauthor{Haverinen_2024} proposed an approach for automating cybersecurity compliance management within DevSecOps pipelines through an open information model that encodes compliance requirements as logic programs, enabling automated policy checks and compliance calculations~\cite{Haverinen_2024}. Their framework-agnostic approach demonstrates technical feasibility of encoding NIST-aligned controls as code---a capability that digital threads could extend by connecting compliance-as-code implementations to authoritative system models. NIST's own DevSecOps guidance, including SP~800-204D on software supply chain security integration within CI/CD pipelines~\cite{NIST_SP_800_204D_2024}, establishes institutional expectations for automated security within development and operational workflows. Yet the academic literature connecting CMMC compliance automation to Digital Engineering remains sparse. The near-absence of peer-reviewed research at this intersection---despite growing practitioner interest and institutional investment---reinforces the contribution of the present investigation: understanding whether Information Assurance professionals perceive value in Digital Engineering capabilities that could address the documentation, traceability, and verification demands these compliance frameworks impose.

NIST's systems security engineering publications establish principles that align with Digital Engineering's integrated approach. Special Publication 800-160 Volume 1 Revision 1 describes a basis for engineering trustworthy secure systems~\cite{Ross_Winstead_McEvilley_2022}. Special Publication 800-160 Volume 2 Revision 1 addresses cyber resiliency considerations~\cite{NIST_SP_800_160_V2_2021}. While these publications reference model-based approaches and traceability requirements, they do not provide specific implementation guidance for Digital Engineering in enterprise IT contexts. Requirements that Digital Engineering could address are established---traceability between security requirements, design decisions, and implementation artifacts; documentation that maintains currency throughout system lifecycles; visibility into system configurations and relationships---yet they do not explicitly connect these requirements to Digital Engineering solutions.

Emerging research on compliance automation demonstrates that the academic community has recognized the inadequacy of manual compliance approaches, even if this recognition has not yet connected to the Digital Engineering discourse. \citeauthor{Joshi_Elluri_2020} developed a semantically rich knowledge graph capturing regulations from GDPR, PCI DSS, ISO 27001, NIST 800-53, and CSA CCM, enabling automated compliance checking for cloud services~\cite{Joshi_Elluri_2020}. Their work, validated against privacy policies of major cloud providers, demonstrates that model-based approaches to compliance are technically feasible and practically valuable. \citeauthor{Banse_2021} presented the Cloud Property Graph, bridging static code analysis and runtime security assessment using an ontology of cloud resources to enable automated identification of misconfigurations and regulatory non-compliance across multi-vendor cloud deployments~\cite{Banse_2021}. \citeauthor{Stojkov_2021} proposed a UML-based model for cross-standard security requirements with explicit mapping to NIST's Open Security Controls Assessment Language (OSCAL) Catalog Model, enabling organizations to track compliance across multiple frameworks simultaneously~\cite{Stojkov_2021}. Most recently, \citeauthor{Koufos_2025} combined attack graphs with OSCAL using compliance-as-code principles to automate risk assessment~\cite{Koufos_2025}.

NIST's own OSCAL initiative represents institutional recognition that manual documentation approaches cannot sustain the accuracy and currency that compliance requires~\cite{NIST_OSCAL_2023}. OSCAL provides standardized machine-readable formats for expressing security control catalogs, baselines, profiles, and assessment results. The initiative aligns with Digital Engineering's emphasis upon machine-readable documentation, and digital thread traceability could connect OSCAL compliance documentation to underlying system configurations, enabling automated verification that documented controls exist as implemented. Yet this integration has not been investigated in the academic literature.

The compliance automation research and the Digital Engineering literature thus develop along parallel tracks that have not converged. Compliance researchers build knowledge graphs, property graphs, and domain-specific languages to automate regulatory assessment. Digital Engineering researchers develop model-based approaches, digital threads, and authoritative sources of truth for complex system management. Both communities address documentation accuracy, traceability, and automated verification. Neither community has systematically explored how integrating their approaches might produce capabilities exceeding what either achieves independently. This unexplored intersection represents a significant research opportunity that the present investigation begins to address by establishing whether the professionals who must implement compliance---IT and Information Assurance practitioners---recognize value in Digital Engineering capabilities.

DevSecOps literature provides additional context for this compliance discussion. \citeauthor{Rajapakse_2022} conducted a systematic review of fifty-four peer-reviewed studies identifying twenty-one challenges and thirty-one solutions in DevSecOps adoption, classified into People, Practices, Tools, and Infrastructure themes~\cite{Rajapakse_2022}. Recommending shift-left security and continuous security assessment, the review identified approaches that align conceptually with the integrated security engineering that MBSE enables. Earlier, \citeauthor{Myrbakken_2017} provided one of the first systematic examinations of DevSecOps, defining the field and characterizing the challenges of integrating security into DevOps practices~\cite{Myrbakken_2017}. These DevSecOps studies establish the problem space---integrating security into development and operational pipelines---without connecting to the Digital Engineering framework that could provide structured methodology for that integration.

\section{IT Service Management: Structural Failures Despite Mature Frameworks}

IT Service Management frameworks assume capabilities that current practices cannot sustain. The Information Technology Infrastructure Library provides comprehensive guidance for aligning IT services with business needs through structured processes, yet ITIL's effectiveness depends upon the accuracy and currency of underlying configuration information---accuracy that organizations consistently fail to achieve~\cite{Cannon_AXELOS_2013}. ITIL 4 reorganized service management practices and introduced the Service Value System concept, acknowledging that tracking configurations across virtual systems, cloud computing, and cybersecurity domains presents challenges, but providing limited guidance on addressing the documentation accuracy challenges that undermine every ITIL process~\cite{ITIL_4_2019}.

Empirical evidence for ITIL implementation challenges proves revealing. \citeauthor{Cook_2021} found resistance to change at twenty-seven percent as the top ITIL implementation challenge~\cite{Cook_2021}. \citeauthor{Marrone_Kolbe_2011} surveyed 491 firms finding that while over ninety percent use ITSM frameworks, little research examines actual benefits realized~\cite{Marrone_Kolbe_2011}. A striking parallel with MBSE evidence emerges: both MBSE and ITIL are widely adopted based upon perceived rather than measured value, and both face adoption challenges rooted in organizational factors rather than technical limitations. Yet no research examines whether integrating Digital Engineering practices with ITIL frameworks could address the persistent challenges that ITIL alone has not resolved.

Configuration Management Database failures provide the most extensively documented evidence of structural inadequacy. Gartner reports an eighty percent failure rate for CMDB implementations~\cite{Gartner_CMDB_2019}. Additional research indicates that ninety-nine percent of organizations using CMDB tooling without addressing data quality gaps will experience visible business disruption~\cite{Gartner_CMDB_2020}. Forrester research finds that less than half of organizations trust the data in their CMDB~\cite{Forrester_CMDB_2020}. Peer-reviewed research corroborates these industry findings: \citeauthor{Hauder_Matthes_Roth_2012} found through a study of 123 enterprise architecture practitioners that manual documentation processes cannot maintain accuracy in dynamic environments~\cite{Hauder_Matthes_Roth_2012}. Data quality statistics reveal the core challenge: sixty percent of data manually input by employees proves inaccurate~\cite{IBM_Data_Quality_2020}.

Recent analysis concludes that the CMDB approach itself has failed~\cite{Forrester_CMDB_Dead_2025}. After decades of implementation attempts across organizations, CMDBs consistently fail to deliver intended value. Failure attribution centers upon structural issues: involving process experts rather than data management professionals, manual data entry that cannot maintain accuracy, and scope creep that renders CMDBs unmanageable. Forrester's research on Application and Infrastructure Dependency Mapping found that fifty-six percent of enterprises report incomplete views of dependencies between applications and underlying infrastructure~\cite{Forrester_AIDM_2018}. Such findings suggest that incremental CMDB improvements cannot resolve inherent approach limitations---a conclusion that opens conceptual space for model-based alternatives that maintain accuracy through automated synchronization rather than manual data entry.

Shadow IT compounds the documentation challenge. Gartner research indicates forty-one percent of employees used shadow IT in 2022, expected to climb to seventy-five percent by 2027, with thirty to forty percent of large companies' IT expenditure representing shadow IT~\cite{Gartner_Shadow_IT_2022}. \citeauthor{Klotz_2019} conducted a systematic review of 126 publications documenting how silos between business units and IT departments drive shadow IT, creating security risks, data inconsistency, and compliance violations~\cite{Klotz_2019}. Systems deployed outside formal governance remain invisible to IT operations and security teams, creating unknown attack surfaces that no documentation process can capture. \citeauthor{Furstenau_2021} examined organizational dynamics when hidden IT systems are discovered, revealing interdepartmental conflicts and governance failures that perpetuate documentation gaps~\cite{Furstenau_2021}. Shadow IT reflects a structural mismatch between IT governance and organizational needs: when official IT processes cannot meet user requirements quickly enough, users acquire solutions independently, creating operational dependencies that official documentation does not capture.

Change management effectiveness depends directly upon documentation accuracy. Industry analysis confirms that reliance upon outdated documentation leads to inaccurate impact assessments~\cite{Freshservice_Change_2021}. Research by \citeauthor{Bokan_Santos_2021} highlights difficulties organizations encounter in maintaining comprehensive security oversight~\cite{Bokan_Santos_2021}. Change management depends upon accurate understanding of system relationships~\cite{iso20000_2018} that current documentation approaches cannot maintain. Every change approved based upon inaccurate documentation represents a potential incident. When impact assessments miss dependencies that exist in operational systems, changes cause unintended effects that consume resources, damage trust in change processes, and create pressure for emergency changes that further degrade documentation accuracy. A self-reinforcing cycle emerges, resistant to process improvement within the document-centric paradigm.

\citeauthor{thompson2019integrating} examined integrating MBSE with IT Service Management, and previous research by \citeauthor{Bonar_Hastings_2024} demonstrated that compliance verification is enhanced by Digital Engineering practices~\cite{thompson2019integrating, Bonar_Hastings_2024}. These preliminary investigations suggest that integration between Digital Engineering and IT Service Management offers value, though comprehensive research remains absent.

\section{Enterprise Visibility: Empirical Evidence of Systemic Failure}\label{sec:enterprise_visibility}

Challenges documented in the preceding sections share common roots in the inability of organizations to maintain accurate, current documentation of their enterprise information systems. Synthesizing peer-reviewed research and industry analysis, this section establishes that these challenges reflect systemic patterns rather than isolated organizational deficiencies, and to provide the empirical foundation for evaluating Digital Engineering as a potential disciplinary response.

Research consistently documents that organizations lack visibility into large portions of their IT environments. \citeauthor{IDC_Exabeam_2023} found that organizations globally can monitor only sixty-six percent of their IT environments, leaving blind spots particularly in cloud deployments~\cite{IDC_Exabeam_2023}. Ponemon Institute's Global Study on Closing the IT Security Gap found that sixty-three percent of security teams lack visibility and control into all user device activity connected to their infrastructure~\cite{Ponemon_Visibility_2023}. Only fifteen percent of respondents in the SANS Institute SOC Survey expressed very high confidence that all devices on their network are discoverable~\cite{SANS_SOC_Survey_2023}. Compounding across organizational boundaries, these visibility gaps Ivanti's State of Cybersecurity Trends Report found that fifty-five percent of organizations maintain security and IT data silos, with sixty-two percent reporting that silos slow security response times~\cite{Ivanti_Cybersecurity_2025}. The Cloud Security Alliance revealed that ninety-five percent of organizations suffered cloud-related breaches in the preceding eighteen months~\cite{CSA_Cloud_Security_2024}. Check Point found that eighty-two percent of enterprises experienced security incidents due to cloud misconfigurations, while sixty-seven percent struggle with limited visibility into cloud infrastructure~\cite{Check_Point_Cloud_2024}.

Peer-reviewed research provides empirical grounding for these industry findings. \citeauthor{Yin_Yuan_Lu_2011} conducted an empirical study examining configuration errors in commercial and open source systems, finding that seventy to eighty-five percent of misconfigurations result from mistakes in setting configuration parameters and that twenty-two to fifty-seven percent of misconfigurations involve configurations external to the examined system~\cite{Yin_Yuan_Lu_2011}. NIST Special Publication 800-128 defines configuration drift as systems deviating from baseline configurations over time through manual interventions, software updates, and environmental factors~\cite{NIST_SP_800_128}. Uptime Institute's Annual Outage Analysis confirms that sixty-four percent of IT system outages occurred because of configuration or change management issues~\cite{Uptime_Outage_2023}. Complementing this finding, the IT Process Institute established that eighty percent of unplanned outages result from ill-planned changes made by administrators or developers~\cite{IT_Process_Institute_2004}---changes that proper dependency documentation would have flagged.

Breach detection times serve as proxy measures for organizational visibility. \citeauthor{IBM_Ponemon_2024} report that the mean time to identify breaches reached two hundred four days, with breaches involving lifecycles exceeding two hundred days costing an average of 5.46 million dollars~\cite{IBM_Ponemon_2024}. A 2025 update found that thirty-five percent of breaches involved shadow data---information stored in unmanaged locations---and forty percent involved data stored across multiple environments that organizations struggle to inventory comprehensively~\cite{IBM_Ponemon_2025}. Taken together, these findings demonstrate that visibility failures directly impact security outcomes in measurable economic terms.

Peer-reviewed research provides theoretical and empirical frameworks for understanding why these failures persist. \citeauthor{Bento_Tagliabue_Lorenzo_2020} conducted a scoping review of forty studies on organizational silos, identifying five conceptualizations and characterizing silo mentality as an absence of systems thinking and organizational vision~\cite{Bento_Tagliabue_Lorenzo_2020}. \citeauthor{Beese_2023} demonstrated through PLS-SEM analysis of 249 information systems managers that IS architecture complexity significantly reduces efficiency, flexibility, transparency, and predictability, and that organizations without adequate enterprise architecture management face compounding architectural degradation~\cite{Beese_2023}. \citeauthor{Kotusev_2019} examined twenty-seven organizations and found that prescribed enterprise architecture artifact lists from frameworks like TOGAF were never empirically validated and do not reflect actual practice---documenting a fundamental gap between what frameworks recommend and what organizations find useful~\cite{Kotusev_2019}. \citeauthor{Kurnia_2021} identified inhibitors to stakeholder engagement in enterprise architecture practice, including organizational silos, political barriers, and poor cross-team communication that directly undermine documentation quality and IT governance effectiveness~\cite{Kurnia_2021}. \citeauthor{Bree_Karger_2022} conducted a systematic literature review organizing enterprise architecture management challenges into six dimensions, with documentation challenges including dearth of automated tools, immature documentation models, and insufficient emphasis on forward-looking documentation~\cite{Bree_Karger_2022}.

Complexity theory provides an explanatory framework for why traditional documentation approaches fail in modern enterprise environments. \citeauthor{Benbya_McKelvey_2006} applied Complex Adaptive Systems theory to information systems development, proposing seven first principles of adaptive success and arguing that if complexity is not managed appropriately, information systems fail~\cite{Benbya_McKelvey_2006}. \citeauthor{Benbya_Nan_Tanriverdi_Yoo_2020} demonstrated that enterprise information systems have reached complexity levels exceeding prior technological generations~\cite{Benbya_Nan_Tanriverdi_Yoo_2020}. Enterprise IT environments exhibit characteristics of complex adaptive systems---numerous interconnected components, emergent behaviors arising from component interactions, continuous change, and unpredictable responses to interventions---that static documentation approaches assume remain stable between documentation updates.

Technical debt research adds further perspective. \citeauthor{Santos_2019} specifically addressed documentation technical debt---problems concerning non-existent, inadequate, or incomplete software project documentation~\cite{Santos_2019}. \citeauthor{Li_Avgeriou_Liang_2015} conducted a systematic mapping study covering ninety-four studies establishing technical debt taxonomy and management frameworks, including documentation debt as a distinct category~\cite{Li_Avgeriou_Liang_2015}. \citeauthor{Junior_Travassos_2022} consolidated perspectives on technical debt across nineteen secondary studies~\cite{Junior_Travassos_2022}. \citeauthor{Kleinwaks_2023} extended technical debt concepts to systems engineering contexts~\cite{Kleinwaks_2023}. Documentation technical debt accumulates when organizations defer documentation updates to prioritize operational activities. Unlike code technical debt, documentation debt remains invisible until documentation is needed for change impact assessment, incident response, or compliance verification---at which point the accumulated debt imposes costs far exceeding the original deferral savings.

Table~\ref{tab:visibilityStatistics} summarizes the evidence documenting enterprise visibility and documentation failures across peer-reviewed and industry research sources.

\begin{table}[H]
  \centering
  \caption{Enterprise Visibility and Documentation Failure Evidence}\label{tab:visibilityStatistics}
  \small
  \begin{tabular}{@{}p{3.0in}p{1.0in}p{2.0in}@{}}
    \toprule
    \textbf{Finding} & \textbf{Statistic} & \textbf{Source} \\
    \midrule
    \multicolumn{3}{@{}c@{}}{\textit{Visibility Gap Metrics}} \\
    \midrule
    IT environment monitorable & 66\% & IDC/Exabeam 2023\cite{IDC_Exabeam_2023} \\
    \midrule
    Security teams lacking device visibility & 63\% & Ponemon Institute 2023\cite{Ponemon_Visibility_2023} \\
    \midrule
    High confidence in device discovery & 15\% & SANS Institute 2023\cite{SANS_SOC_Survey_2023} \\
    \midrule
    Organizations with security/IT silos & 55\% & Ivanti 2025\cite{Ivanti_Cybersecurity_2025} \\
    \midrule
    \multicolumn{3}{@{}c@{}}{\textit{Configuration Management Failures}} \\
    \midrule
    CMDB implementation failure rate & 80\% & Gartner Research\cite{Gartner_CMDB_2019,Gartner_CMDB_2020} \\
    \midrule
    Outages from configuration issues & 64\% & Uptime Institute 2023\cite{Uptime_Outage_2023} \\
    \midrule
    Misconfigurations from parameter errors & 70-85\% & Yin et al. 2011\cite{Yin_Yuan_Lu_2011} \\
    \midrule
    Unplanned outages from ill-planned changes & 80\% & IT Process Institute\cite{IT_Process_Institute_2004} \\
    \midrule
    \multicolumn{3}{@{}c@{}}{\textit{Shadow IT and Undocumented Assets}} \\
    \midrule
    Shadow IT as percentage of IT spend & 30-40\% & Gartner Research\cite{Gartner_Shadow_IT_2022} \\
    \midrule
    Cloud services vs. IT estimates & 15-22x higher & Cisco 2016\cite{Cisco_Cloud_Services_2016} \\
    \midrule
    Employees using shadow IT (2022) & 41\% & Gartner Research\cite{Gartner_Shadow_IT_2022} \\
    \midrule
    Projected shadow IT usage (2027) & 75\% & Gartner Research\cite{Gartner_Shadow_IT_2022} \\
    \midrule
    \multicolumn{3}{@{}c@{}}{\textit{Security Impact Metrics}} \\
    \midrule
    Mean time to identify breach & 204 days & IBM/Ponemon 2024\cite{IBM_Ponemon_2024} \\
    \midrule
    Cloud breaches from misconfigurations & 82\% & Check Point 2024\cite{Check_Point_Cloud_2024} \\
    \midrule
    Organizations with cloud breaches (18 mo) & 95\% & CSA 2024\cite{CSA_Cloud_Security_2024} \\
    \midrule
    Projected preventable cloud breaches (2027) & 99\% & Gartner Research\cite{Gartner_Cloud_Misconfiguration_2024} \\
    \bottomrule
  \end{tabular}
\end{table}

The convergence of peer-reviewed empirical research and industry analysis establishes that enterprise visibility and documentation failures represent a systemic challenge rather than isolated organizational deficiencies. Traditional documentation approaches---manual configuration tracking, periodic documentation updates, static architecture diagrams---cannot maintain accuracy in environments characterized by continuous change, complex dependencies, and organizational silos.

A methodological observation regarding the evidentiary sources presented in this section warrants acknowledgment. Enterprise visibility evidence draws substantially upon industry analyst reports and practitioner surveys alongside peer-reviewed academic research. Reliance upon industry gray literature reflects a characteristic of the research domain rather than an analytical preference: the operational challenges of enterprise IT documentation and visibility have received considerably more attention from industry analysts and practitioner communities than from academic researchers. Relative scarcity of peer-reviewed empirical studies quantifying CMDB failure rates, shadow IT prevalence, and breach detection timelines itself constitutes evidence of the research gap this dissertation addresses. Where peer-reviewed evidence corroborates industry findings---as with the work of \citeauthor{Hauder_Matthes_Roth_2012} on documentation accuracy, \citeauthor{Santos_2019} on documentation technical debt, \citeauthor{Beese_2023} on architecture complexity, and \citeauthor{Kotusev_2019} on enterprise architecture practice---the convergence strengthens confidence in the broader pattern. The industry evidence is presented not as a substitute for academic rigor but as the best available evidence for phenomena that academic research has not yet systematically investigated.

\section{Research Gaps and Theoretical Framework}

A clear pattern emerges from the systematic literature review: frameworks and compliance requirements assume documentation and visibility capabilities that organizations demonstrably lack, while Digital Engineering methodologies that could address these capabilities remain confined within a discipline that enterprise IT and Information Assurance have not engaged. Synthesizing the findings into a theoretical framework, this section documents the research gaps that this investigation begins to address.

The academic research gap is pronounced. Systematic literature reviews examining MBSE consistently find no research addressing enterprise IT infrastructure or Information Assurance applications beyond the preliminary reference model by \citeauthor{Bonar_Hastings_2024}~\cite{Bonar_Hastings_2024} and the conceptual DevSecOps framework by \citeauthor{CMU_SEI_DevSecOps_2023}~\cite{CMU_SEI_DevSecOps_2023}. Persistence of this gap despite explicit requirements in compliance frameworks for capabilities that Digital Engineering provides demands explanation. Digital Engineering immaturity cannot account for it---defense and aerospace have employed Digital Engineering successfully for years. Tool unavailability cannot account for it---MBSE tools, digital twin platforms, and PLM systems have existed for over a decade (MBSE) to decades (digital twin, PLM, \& authoritative traceability). Rather, the gap reflects a disciplinary boundary: systems engineering and IT have evolved as separate disciplines with limited cross-pollination, creating what \citeauthor{Henderson_Salado_2024_Org} would characterize as low interconnectedness between organizational units---precisely the structural condition their research associates with impeded adoption~\cite{Henderson_Salado_2024_Org}.

Standards bodies have recognized enterprise applicability of systems engineering approaches. The Unified Architecture Framework provides viewpoints applicable to enterprise IT\@. NIST publications require enterprise architecture capabilities for compliance. ITIL requires visibility and documentation that model-based approaches could provide. Yet academic research has not examined practical application. This standards-research disconnect leaves practitioners without empirical guidance for applying available standards to enterprise IT challenges.

Table~\ref{tab:researchGaps} summarizes the research gaps identified across the literature domains examined in this review.

\begin{table}[htbp]
  \centering
  \caption{Research Gaps Within Corpus of Knowledge}\label{tab:researchGaps}
  \small
  \begin{tabular}{@{}p{1.8in}p{2.1in}p{2.2in}@{}}
    \toprule
    \textbf{Domain} & \textbf{Gap Description} & \textbf{Research Implication} \\
    \midrule
    MBSE for Enterprise IT & One study applying MBSE to enterprise IT, none for IA program management & Foundation research required \\
    \midrule
    MBSE Adoption Evidence & Adoption studies limited to SE populations; no data from IT or IA professionals & Perception measurement needed in new populations \\
    \midrule
    Digital Threads & No research on traceability for IT/IA contexts & Conceptual validation needed \\
    \midrule
    Digital Twin & Growing security application research but no enterprise IT integration studies & Application studies needed \\
    \midrule
    ITSM Integration & No frameworks integrating DE with ITIL & Integration research required \\
    \midrule
    Compliance Automation & Model-based compliance research and DE research develop on parallel tracks without convergence & Integration research needed \\
    \midrule
    Open Source & No academic validation for enterprise IT & Evaluation research needed \\
    \midrule
    Professional Perceptions & Unknown awareness and perceived value among IT/IA professionals & This research addresses \\
    \bottomrule
  \end{tabular}
\end{table}

Based upon the systematic literature review, this research adopts a theoretical framework integrating Digital Engineering principles with established Information Assurance and IT Service Management practices. Positing that Digital Engineering represents a disciplinary approach with demonstrated value in defense and aerospace contexts whose capabilities align with gaps that have persisted in enterprise IT despite decades of framework development and organizational investment, this framework does not assert that Digital Engineering will resolve these gaps in enterprise IT contexts---that remains an empirical question. Rather, it identifies structural correspondences between demonstrated Digital Engineering capabilities and documented enterprise IT challenges, establishing the theoretical basis for investigating whether practitioners recognize these correspondences.

The persistent failures documented in IT Service Management and Information Assurance practices share common root causes that Digital Engineering practices are designed to address. Organizations struggle with documentation accuracy because traditional approaches rely upon manual processes disconnected from operational systems. Organizations fail to maintain traceability because document-centric methods cannot sustain verified connections as systems evolve. Organizations lack visibility because static artifacts cannot represent dynamic system states. Digital Engineering's demonstrated ability to address these root causes in defense and aerospace contexts motivates investigation of whether similar benefits may transfer to enterprise IT environments.

The DoD Digital Engineering Strategy defines the authoritative source of truth as ``a single source of data and models'' that provides ``a definitive technical baseline'' for programs~\cite{DoD_DE_Strategy_2018}. Current IT and Information Assurance practices lack authoritative sources of truth, instead maintaining multiple disconnected documentation artifacts that diverge over time. Digital threads establish and maintain authoritative traceability throughout system lifecycles, addressing the gap between security requirements, control implementations, and compliance evidence. Digital twin capabilities enable organizations to simulate system behavior, test proposed changes, and analyze scenarios without affecting production systems. Model-based approaches maintain synchronization with operational systems, providing the visibility that manual documentation cannot sustain.

The theoretical framework acknowledges several limitations. First, it extrapolates from Digital Engineering value demonstrated in aerospace and defense to anticipated value in enterprise IT contexts. Whether benefits demonstrated for physical systems transfer to logical information systems remains unvalidated. Second, it assumes that Digital Engineering tools and methodologies can be adapted for enterprise IT contexts. \citeauthor{Mazeika_Butleris_2020} documented that current MBSE methods inadequately address security requirements~\cite{Mazeika_Butleris_2020}, suggesting adaptation requirements may exceed anticipated effort. Third, the framework does not address organizational change management, workforce development, or cultural transformation requirements. The adoption barriers documented by \citeauthor{Henderson_McDermott_Salado_2024}---including middle management resistance, awareness deficits, and the need for dedicated MBSE teams~\cite{Henderson_McDermott_Salado_2024}---would likely manifest with greater intensity in domains unfamiliar with systems engineering practices. Fourth, the framework focuses upon potential benefits without comprehensive analysis of costs or implementation challenges. Cost-benefit analysis requires empirical data this research does not collect.

\section{Chapter Summary}

This literature review has examined the current body of knowledge across interconnected domains relevant to applying Digital Engineering methodologies to Information Assurance and IT Service Management, advancing three interrelated arguments that position the present research within the academic conversation.

First, the evidence base for MBSE value, while compelling in individual cases such as the eighteen percent efficiency improvement and nine percent defect reduction documented by \citeauthor{Rogers_Mitchell_2021}~\cite{Rogers_Mitchell_2021}, remains disproportionately reliant upon perceived rather than measured outcomes. \citeauthor{Henderson_Salado_2021} found that approximately two-thirds of claimed benefits lack empirical measurement~\cite{Henderson_Salado_2021}. This evidence deficit creates rational hesitation among organizations outside the systems engineering discipline, where institutional experience cannot compensate for the measurement gap.

Second, adoption research demonstrates that perceptions govern adoption decisions more powerfully than objective technical merit. \citeauthor{Call_2024} showed that perceived complexity and compatibility barriers impede adoption even among systems engineering professionals who recognize MBSE's relative advantage~\cite{Call_2024}. \citeauthor{Henderson_McDermott_Salado_2024} found that twenty-two percent of systems engineering professionals cannot clearly define MBSE~\cite{Henderson_McDermott_Salado_2024}. If awareness deficits and perception barriers persist within the originating discipline, their magnitude among IT and Information Assurance professionals---who operate outside the systems engineering discipline entirely---demands empirical measurement rather than assumption.

Third, compliance frameworks and IT service management standards require capabilities that current practices demonstrably fail to provide, while Digital Engineering offers those capabilities in adjacent domains. Enterprise visibility evidence---sixty-six percent IT environment monitoring, eighty percent CMDB failure rates, 204-day average breach detection times---documents the scope of failure. Growing recognition within the compliance automation literature demonstrates that model-based approaches can address documentation and traceability requirements. Yet no research connects these parallel tracks by examining Digital Engineering as an integrated solution for enterprise IT and Information Assurance challenges.

Developed from this synthesis, the theoretical framework posits that structural correspondences between demonstrated Digital Engineering capabilities and documented enterprise IT challenges warrant investigation of whether practitioners recognize these correspondences. Contributing by investigating whether IT and Information Assurance professionals perceive value in Digital Engineering capabilities, the research. If professionals perceive value, findings justify subsequent implementation research. If professionals do not perceive value despite documented challenges, findings challenge the theoretical premise and identify barriers requiring address before adoption can occur.

Chapter 3 presents the research methodology employed to investigate professional awareness and perceptions of Digital Engineering capabilities. The methodology utilizes a quantitative survey-based approach following a systems engineering lifecycle to ensure rigor and traceability throughout the research process.